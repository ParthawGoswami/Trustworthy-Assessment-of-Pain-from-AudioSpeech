{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary: GaleMed Insights – A RAG-Based Medical Chatbot Using Pinecone and LLMs\n",
    "\n",
    "---\n",
    "## Project Overview\n",
    "GaleMed Insights is an advanced medical chatbot built using a Retrieval-Augmented Generation (RAG) framework, leveraging the wealth of medical knowledge contained in The Gale Encyclopedia of Medicine, which spans over 637 pages. By combining PDF processing, semantic embedding, and vector databases, GaleMed Insights offers accurate, detailed, and user-friendly responses to medical inquiries. This makes it a valuable and accessible resource for individuals seeking trustworthy health information.\n",
    "\n",
    "With its foundation in The Gale Encyclopedia of Medicine, GaleMed Insights ensures users can access well-researched, vetted content on a wide range of medical topics, from conditions and treatments to best healthcare practices. The chatbot empowers users to easily navigate complex medical concepts and obtain clear, relevant answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use RAG-Based Methodology Instead of Just LLMs?\n",
    "Using a Retrieval-Augmented Generation (RAG) methodology offers significant advantages over traditional large language models (LLMs) alone:\n",
    "\n",
    "#### Higher Accuracy and Reliability:\n",
    "GaleMed Insights retrieves factual information directly from The Gale Encyclopedia of Medicine, ensuring that responses are factually accurate and relevant. This drastically reduces the risk of generating incorrect information (known as \"hallucinations\"), which is a common issue when using LLMs on their own.\n",
    "\n",
    "#### Targeted Knowledge Base:\n",
    "Instead of relying on broad datasets, GaleMed Insights focuses on a specific medical knowledge base, ensuring that users receive expert, specialized responses. This contrasts with general LLMs, which may provide superficial or irrelevant answers to health-related queries.\n",
    "\n",
    "#### Contextual Relevance:\n",
    "Through chunking and semantic embedding, GaleMed Insights retrieves the most relevant information while maintaining the necessary context, which leads to clearer and more coherent responses tailored to the user's needs.\n",
    "\n",
    "#### Efficient Memory Management:\n",
    "Dividing the text into chunks avoids running into token limits of LLMs, allowing the model to process large documents effectively without losing important details from the encyclopedia.\n",
    "\n",
    "#### Improved User Experience:\n",
    "With precise, well-formatted responses, GaleMed Insights enhances the readability and accessibility of complex medical information, making it easier for users to understand and act on the information provided.\n",
    "\n",
    "---\n",
    "## Why This Project?\n",
    "The GaleMed Insights project is designed to address the growing need for reliable, accurate, and accessible medical information. In a world where misinformation spreads easily, especially on health-related topics, GaleMed Insights offers a trusted source of knowledge from The Gale Encyclopedia of Medicine. Key reasons for developing this project include:\n",
    "\n",
    "#### Improving Health Literacy:\n",
    "GaleMed Insights empowers individuals to make informed decisions about their health by providing clear and factual medical information.\n",
    "\n",
    "#### Supporting Healthcare Professionals:\n",
    "The chatbot serves as a quick reference tool for healthcare providers, giving them rapid access to accurate medical information to assist in patient care.\n",
    "\n",
    "#### Easy Knowledge Base Updates:\n",
    "As medical knowledge evolves, GaleMed Insights can easily be updated with new data, ensuring the chatbot remains current and provides the latest medical insights.\n",
    "\n",
    "---\n",
    "## Steps: \n",
    "### Document Reading with PyPDF:\n",
    "The project utilizes the pyPDF library to read and extract data from The Gale Encyclopedia of Medicine. This library efficiently handles large documents, facilitating the structured extraction of valuable medical information.\n",
    "\n",
    "### Data Chunking:\n",
    "After extracting the data, it is crucial to divide the information into manageable chunks. Chunking helps to:\n",
    "\n",
    "- Improve retrieval efficiency by enabling quick access to relevant sections.\n",
    "\n",
    "- Enhance context provided in responses, allowing the model to generate more meaningful answers.\n",
    "\n",
    "- Mitigate issues related to the maximum token limits of language models by ensuring that only concise and relevant information is processed at any given time.\n",
    "\n",
    "### Creating Semantic Embeddings:\n",
    "\n",
    "- To facilitate meaningful queries and responses, we employ the HuggingFaceEmbeddings model (sentence-transformers/all-MiniLM-L6-v2). This model generates semantic embeddings, which capture the context and meaning of the text beyond surface-level words. These embeddings enable GaleMed to comprehend user queries more effectively and retrieve the most relevant information.\n",
    "\n",
    "### Building a Vector Database with Pinecone:\n",
    "The next step involves creating a vector database using Pinecone. We load the chunked and embedded data into Pinecone, establishing a knowledge base that can efficiently manage user queries by identifying similar vectors and retrieving pertinent information.\n",
    "\n",
    "### Testing Queries:\n",
    "Once the knowledge base is established, we conduct test queries to evaluate the performance of GaleMed. This testing phase ensures that the chatbot retrieves accurate and relevant information effectively, confirming the effectiveness of the RAG approach.\n",
    "\n",
    "### Utilizing LLM for Response Formatting:\n",
    "To enhance the clarity and readability of the information returned to users, we utilize a large language model (LLM) based on the LLaMA architecture (CTransformers). This offline version processes the extracted data and formats it into coherent responses, making complex medical information accessible and understandable.\n",
    "\n",
    "### Flask Application Development:\n",
    "Finally, we aim to develop a Flask web application that serves as the interface for GaleMed. This application will allow users to interact with the chatbot seamlessly, posing questions and receiving comprehensive answers based on the medical encyclopedia.\n",
    "\n",
    "---\n",
    "## Real-World Use Cases of RAG Methodology in Other Fields\n",
    "This RAG-based approach is not only useful in healthcare but also has real-world applications in various fields, such as:\n",
    "\n",
    "#### Legal Industry:\n",
    "Legal research chatbots can use RAG to pull up relevant laws, precedents, and case summaries from specific legal databases, providing accurate and contextual legal advice.\n",
    "\n",
    "#### Customer Support:\n",
    "Companies can use RAG to power customer service bots that retrieve relevant product information and troubleshooting steps from their internal knowledge bases, enhancing customer experience with quicker and more accurate responses.\n",
    "\n",
    "#### Education:\n",
    "RAG-based educational bots can assist students by providing targeted explanations and detailed answers based on textbooks, enhancing their learning experiences.\n",
    "\n",
    "#### Technical Documentation:\n",
    "Tech companies can use RAG to help engineers and developers quickly retrieve information from large technical manuals or knowledge bases to solve problems efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "By using a RAG framework such as GaleMed Insights, we can enable and ensure that users get precise, relevant information from a trusted source, improving the overall accuracy and reliability of responses across various industries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data got from https://www.academia.edu/32752835/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND_EDITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/.venv/lib/python3.9/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone as LangchainPinecone  # Using alias for LangChain Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical-vector exists.\n",
      "An error occurred while creating embeddings: name 'text_chunks' is not defined\n"
     ]
    }
   ],
   "source": [
    "#Initializing index name and the Pinecone\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_6fBgkP_T5NkSGZ1yJhXMoUM9i7mCRh5h396peZEQqXXiwFkGk58xi2QCBAXvdVCwVVe7aE\"\n",
    "\n",
    "index_name=\"medical-vector\"\n",
    "\n",
    "# Initialize Pinecone with optional parameters\n",
    "source\n",
    "import os\\n\n",
    "# Set the token in the environment so huggingface_hub will use it non-interactively.\\n\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = \"REDACTED_TOKEN\"\\n\n",
    "# Try to verify token availability without calling notebook widgets (no ipywidgets needed)\\n\n",
    "try:\n",
    "    from huggingface_hub import HfApi\\n\n",
    "    api = HfApi()\\n\n",
    "    # We won't print the token; just confirm we set it.\\n\n",
    "    print('Hugging Face token set in environment; proceeding without interactive login.')\\n\n",
    "except Exception as e:\n",
    "    print('Could not import huggingface_hub or verify token:', e)\\n\n",
    "    print('If needed, install huggingface_hub via `pip install huggingface_hub` or authenticate via CLI: `huggingface-cli login`.')\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 384,\n",
       "              'host': 'medical-vector-otl5o30.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'medical-vector',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_6fBgkP_T5NkSGZ1yJhXMoUM9i7mCRh5h396peZEQqXXiwFkGk58xi2QCBAXvdVCwVVe7aE\")\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores are there in the Pinecone that we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 988 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 988 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 147 0 (offset 0)\n",
      "Ignoring wrong pointing object 607 0 (offset 0)\n",
      "Ignoring wrong pointing object 612 0 (offset 0)\n",
      "Ignoring wrong pointing object 614 0 (offset 0)\n",
      "Ignoring wrong pointing object 616 0 (offset 0)\n",
      "Ignoring wrong pointing object 631 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 147 0 (offset 0)\n",
      "Ignoring wrong pointing object 607 0 (offset 0)\n",
      "Ignoring wrong pointing object 612 0 (offset 0)\n",
      "Ignoring wrong pointing object 614 0 (offset 0)\n",
      "Ignoring wrong pointing object 616 0 (offset 0)\n",
      "Ignoring wrong pointing object 631 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "extracted_data = load_pdf(\"/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/Medical_books/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of my chunk: 18036\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gx/_792gs0d1p5_1_15m1lwtn9m0000gn/T/ipykernel_88809/1337643473.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Data into Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical-vector exists.\n"
     ]
    }
   ],
   "source": [
    "#Initializing index name and the Pinecone\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_6fBgkP_T5NkSGZ1yJhXMoUM9i7mCRh5h396peZEQqXXiwFkGk58xi2QCBAXvdVCwVVe7aE\"\n",
    "\n",
    "index_name=\"medical-vector\"\n",
    "\n",
    "# Initialize Pinecone with optional parameters\n",
    "try:\n",
    "    pc = Pinecone(\n",
    "        api_key=os.environ.get(\"PINECONE_API_KEY\"),\n",
    "        proxy_url=None,            # Example optional parameter\n",
    "        proxy_headers=None,        # Example optional parameter\n",
    "        ssl_ca_certs=None,        # Example optional parameter\n",
    "        ssl_verify=True,  # Example optional parameter, usually set to True\n",
    "    )\n",
    "    \n",
    "    time.sleep(2)  # Optional sleep to ensure initialization completes\n",
    "\n",
    "    # Check if the index exists\n",
    "    indexes = pc.list_indexes()  # List of index names\n",
    "    index_names = indexes.names()  # Get only the names of the indexes\n",
    "\n",
    "    if index_name not in index_names:\n",
    "        print(f'{index_name} does not exist')\n",
    "        # Optionally, create a new index\n",
    "        # Uncomment the following line to create the index\n",
    "        # pc.create_index(name=index_name, dimension=384, metric='cosine')\n",
    "    else:\n",
    "        print(f'{index_name} exists.')\n",
    "\n",
    "    # Connect to the existing index\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while checking indexes: {e}\")\n",
    "\n",
    "# Embedding the text chunks and storing them in Pinecone\n",
    "#try:\n",
    "#    docsearch = LangchainPinecone.from_texts(\n",
    "#        texts=[t.page_content for t in text_chunks],  # Assuming `text_chunks` is a list of text splits\n",
    "#        embedding=embeddings,  # Embedding model instance\n",
    "#        index_name=index_name\n",
    "#    )\n",
    "#except Exception as e:\n",
    "#    print(f\"An error occurred while creating embeddings: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result [Document(metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 135.0, 'page_label': '136', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': '/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/Medical_book.pdf', 'total_pages': 637.0}, page_content='Purpose\\nAllergy is a reaction of the immune system. Nor-\\nmally, the immune system responds to foreign microor-\\nganisms and particles, like pollen or dust, by producing\\nspecific proteins called antibodies that are capable of\\nbinding to identifying molecules, or antigens, on the\\nforeign organisms. This reaction between antibody and\\nantigen sets off a series of reactions designed to protect\\nthe body from infection. Sometimes, this same series of'), Document(metadata={}, page_content='Purpose\\nAllergy is a reaction of the immune system. Nor-\\nmally, the immune system responds to foreign microor-\\nganisms and particles, like pollen or dust, by producing\\nspecific proteins called antibodies that are capable of\\nbinding to identifying molecules, or antigens, on the\\nforeign organisms. This reaction between antibody and\\nantigen sets off a series of reactions designed to protect\\nthe body from infection. Sometimes, this same series of'), Document(metadata={}, page_content='reaction. Allergic rhinitis is characterized by an itchy,\\nrunny nose, often with a scratchy or irritated throat due\\nto post-nasal drip. Inflammation of the thin membrane\\ncovering the eye (allergic conjunctivitis) causes redness,\\nirritation, and increased tearing in the eyes. Asthma caus-\\nes wheezing, coughing, and shortness of breath. Symp-\\ntoms of food allergies depend on the tissues most sensi-\\ntive to the allergen and whether the allergen spread sys-')]\n"
     ]
    }
   ],
   "source": [
    "index_name=\"medical-vector\"\n",
    "#If we already have an index we can load it like this\n",
    "docsearch=LangchainPinecone.from_existing_index(index_name, embeddings)\n",
    "\n",
    "query = \"What are Allergies\"\n",
    "\n",
    "docs=docsearch.similarity_search(query, k=3)\n",
    "\n",
    "print(\"Result\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result [Document(metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': '/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'), Document(metadata={}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'), Document(metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': '/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/Medical_book.pdf', 'total_pages': 637.0}, page_content='milk thistle (Silybum marianum), and with nutrients such\\nas essential fatty acids, vitamin B complex, zinc, vitamin\\nA, and chromium is also recommended. Chinese herbal\\nremedies used for acne include cnidium seed ( Cnidium\\nmonnieri) and honeysuckle flower ( Lonicera japonica ).\\nWholistic physicians or nutritionists can recommend the\\nproper amounts of these herbs.\\nPrognosis\\nAcne is not curable, although long-term control is\\nachieved in up to 60% of patients treated with')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Cure for acne?\"\n",
    "\n",
    "docs=docsearch.similarity_search(query, k=3)\n",
    "\n",
    "print(\"Result\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result [Document(metadata={}, page_content='of the brain. They can rupture, causing subarachnoid hemorrhage.CLINICAL APPROACHHe a da c he  i s  one  of the  mos t c ommon c ompl a i nts  of pa ti e nts  i n me di c a l  pra c ti c e .  It periodically afflicts 90% of adults, and almost 25% have recurrent severe head-aches. As with many common symptoms, a broad range of conditions, from trivial to life-threatening, might be responsible. The majority of patients presenting with headache have tension-type, migraine, or cluster; however,'), Document(metadata={}, page_content='A 5 9 -ye a r-o ld  w o m a n  co m e s t o  yo u r clin ic b e ca u se  sh e  is co n ce rn e d  t h a t  sh e  might have a brain tumor. She has had a fairly severe headache for the last  3 weeks (she rates it as an 8 on a scale of 1-10). She describes the pain as constant, occasionally throbbing but mostly a dull ache, and localized to the right side of her head. She thinks the pain is worse at night, especially when she lies with that side of her head on the pillow. She has had no nausea,'), Document(metadata={}, page_content='T able 38–1 • RED FLAGS FOR SERIOUS HEADACHE DISORDERSAn y o f t h e s e  fin d in g s  s h o u ld  p ro m p t  fu r t h e r in ve s t ig a t io n , in c lu d in g  b ra in  im a g in g  (CT o r MRI):•\\u2002Systemic symptoms, illness, or condition (fever, cancer, HIV  or other immunocompromised state)•\\u2002\\u2002Neurologic signs or symptoms (altered mental status, loss of consciousness, focal neurologic signs, seizures, meningismus)•\\u2002\\u2002Onset is new (especially age >40) or sudden (thunderclap headache)•\\u2002\\u2002Other')]\n"
     ]
    }
   ],
   "source": [
    "query = \"I have pain in my head\"\n",
    "\n",
    "docs=docsearch.similarity_search(query, k=3)\n",
    "\n",
    "print(\"Result\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='of the brain. They can rupture, causing subarachnoid hemorrhage.CLINICAL APPROACHHe a da c he  i s  one  of the  mos t c ommon c ompl a i nts  of pa ti e nts  i n me di c a l  pra c ti c e .  It periodically afflicts 90% of adults, and almost 25% have recurrent severe head-aches. As with many common symptoms, a broad range of conditions, from trivial to life-threatening, might be responsible. The majority of patients presenting with headache have tension-type, migraine, or cluster; however,'),\n",
       " Document(metadata={}, page_content='A 5 9 -ye a r-o ld  w o m a n  co m e s t o  yo u r clin ic b e ca u se  sh e  is co n ce rn e d  t h a t  sh e  might have a brain tumor. She has had a fairly severe headache for the last  3 weeks (she rates it as an 8 on a scale of 1-10). She describes the pain as constant, occasionally throbbing but mostly a dull ache, and localized to the right side of her head. She thinks the pain is worse at night, especially when she lies with that side of her head on the pillow. She has had no nausea,'),\n",
       " Document(metadata={}, page_content='T able 38–1 • RED FLAGS FOR SERIOUS HEADACHE DISORDERSAn y o f t h e s e  fin d in g s  s h o u ld  p ro m p t  fu r t h e r in ve s t ig a t io n , in c lu d in g  b ra in  im a g in g  (CT o r MRI):•\\u2002Systemic symptoms, illness, or condition (fever, cancer, HIV  or other immunocompromised state)•\\u2002\\u2002Neurologic signs or symptoms (altered mental status, loss of consciousness, focal neurologic signs, seizures, meningismus)•\\u2002\\u2002Onset is new (especially age >40) or sudden (thunderclap headache)•\\u2002\\u2002Other')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result [Document(metadata={}, page_content='has been tearful on occasions. He has lost interest in his hobbies and has not attempted to \\nhave sexual intercourse with his wife. He feels very tired and has problems concentrating \\nwhen watching the television or reading the newspaper. He has lost about half a stone since \\nthe operation despite the lack of physical exercise. He admits to suicidal thoughts, but denies \\nany concrete plans. Asked to compare his emotional state with a previous episode of depres -'), Document(metadata={}, page_content='sons or objects outside the self that persists despite\\nthe facts.\\nDepression—A state of being depressed marked\\nespecially by sadness, inactivity, difficulty with\\nthinking and concentration, a significant increase or\\ndecrease in appetite and time spent sleeping, feel-\\nings of dejection and hopelessness, and sometimes\\nsuicidal thoughts or an attempt to commit suicide.\\nGlucocorticoid—Any of a group of corticosteroids\\n(as hydrocortisone or dexamethasone) that are'), Document(metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 315.0, 'page_label': '316', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': '/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/Medical_book.pdf', 'total_pages': 637.0}, page_content='sons or objects outside the self that persists despite\\nthe facts.\\nDepression—A state of being depressed marked\\nespecially by sadness, inactivity, difficulty with\\nthinking and concentration, a significant increase or\\ndecrease in appetite and time spent sleeping, feel-\\nings of dejection and hopelessness, and sometimes\\nsuicidal thoughts or an attempt to commit suicide.\\nGlucocorticoid—Any of a group of corticosteroids\\n(as hydrocortisone or dexamethasone) that are')]\n"
     ]
    }
   ],
   "source": [
    "query = \"I am sad all the time\"\n",
    "\n",
    "docs=docsearch.similarity_search(query, k=3)\n",
    "\n",
    "print(\"Result\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result [Document(metadata={}, page_content='diagnosed mild depression and offered therapy, but the girl failed to attend further sessions \\nand was subsequently discharged back to the GP.\\nExamination\\nOn examination, the girl is well-groomed, quiet with her gaze lowered. Her answers to ques -\\ntions on her well-being are monosyllabic. She denies suicidal intent or plans to harm herself. \\nEnquiries on her daily activities are unfruitful as she does not engage in any conversation.'), Document(metadata={}, page_content='knees and wrists are painful at all times and he has increasing difficulty in doing his work. \\nHe is feeling very exhausted and tired, his mood is low and he becomes irritable very eas-\\nily. He has been crying unprovoked. He admits to problems falling asleep at night and early \\nmorning waking. He has daily thoughts of life not being worth living, but denies any suicidal \\nthoughts or intent. He has suffered two bereavements recently: a good friend dying after a'), Document(metadata={}, page_content='has been tearful on occasions. He has lost interest in his hobbies and has not attempted to \\nhave sexual intercourse with his wife. He feels very tired and has problems concentrating \\nwhen watching the television or reading the newspaper. He has lost about half a stone since \\nthe operation despite the lack of physical exercise. He admits to suicidal thoughts, but denies \\nany concrete plans. Asked to compare his emotional state with a previous episode of depres -')]\n"
     ]
    }
   ],
   "source": [
    "query = \"What to do if you are sad?\"\n",
    "\n",
    "docs=docsearch.similarity_search(query, k=3)\n",
    "\n",
    "print(\"Result\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result [Document(metadata={}, page_content='yourself.\\nKey Points'), Document(metadata={}, page_content='subsequent career.\\nP John Rees\\nJames Pattison\\nGwyn Williams'), Document(metadata={}, page_content='xv')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is superman?\"\n",
    "\n",
    "docs=docsearch.similarity_search(query, k=3)\n",
    "\n",
    "print(\"Result\", \n",
    "      docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Prompt Template and testing the model with retreival and LLM Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MedGemma LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.9/site-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.9/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.9/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.9/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.9/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.9/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.9/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached Hugging Face token file; will attempt to use cached credentials (not displayed).\n",
      "Authenticated as: ParthawGoswami\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face auth verification: check env, cached token, and whoami() non-interactively\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import HfApi\n",
    "except Exception as e:\n",
    "    print('huggingface_hub not installed:', e)\n",
    "    print('Install with: pip install huggingface_hub')\n",
    "else:\n",
    "    # 1) Check env var first\n",
    "    token = os.environ.get('HUGGINGFACE_HUB_TOKEN')\n",
    "    if token:\n",
    "        print('Using HUGGINGFACE_HUB_TOKEN from environment (not displayed).')\n",
    "    else:\n",
    "        # 2) Check known cache locations for token saved by CLI (may vary)\n",
    "        cached = None\n",
    "        possible = [Path.home()/'.cache'/'huggingface'/'token', Path.home()/'.cache'/'huggingface'/'stored_tokens']\n",
    "        for p in possible:\n",
    "            try:\n",
    "                if p.exists():\n",
    "                    cached = p.read_text(errors='ignore')\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if cached:\n",
    "            print('Found cached Hugging Face token file; will attempt to use cached credentials (not displayed).')\n",
    "        else:\n",
    "            print('No HUGGINGFACE_HUB_TOKEN env var and no cached token file found.\\nYou can run `huggingface-cli login` in your terminal or set HUGGINGFACE_HUB_TOKEN env var.')\n",
    "    # 3) Attempt to call whoami() to verify auth (this will use env or cached credentials)\n",
    "    try:\n",
    "        api = HfApi()\n",
    "        who = api.whoami()\n",
    "        # whoami returns a dict-like object with user info; print a friendly identifier\n",
    "        name = None\n",
    "        if isinstance(who, dict):\n",
    "            name = who.get('name') or who.get('login')\n",
    "        elif hasattr(who, 'get'):\n",
    "            name = who.get('name') or who.get('login')\n",
    "        else:\n",
    "            name = str(who)\n",
    "        print('Authenticated as:', name)\n",
    "    except Exception as e:\n",
    "        print('Could not verify Hugging Face authentication with whoami():', e)\n",
    "        print('If you just ran `huggingface-cli login` in the terminal, restart the kernel or set HUGGINGFACE_HUB_TOKEN in this notebook session:')\n",
    "        print('  export HUGGINGFACE_HUB_TOKEN=your_token_here')\n",
    "        print('Then re-run this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.91it/s]\n",
      "\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HFAdapter created with text-generation pipeline on device=-1; llm is ready\n"
     ]
    }
   ],
   "source": [
    "# Build a transformers pipeline that LangChain supports (text-generation / text2text-generation)\n",
    "# NOTE: LangChain's HuggingFacePipeline does NOT support the `image-text-to-text` task.\n",
    "# NOTE: This model can be large; on macOS MPS it's easy to OOM. Default to CPU here for stability.\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        # Best-effort cleanup before (re)loading models\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use CPU device (-1) to avoid MPS OOM.\n",
    "# If you have enough VRAM and want MPS, set DEVICE=0 and restart kernel.\n",
    "DEVICE = -1\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"google/medgemma-4b-it\",\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "class HFAdapter:\n",
    "    \"\"\"Small adapter so notebook code can call llm(prompt) or llm.invoke(prompt).\"\"\"\n",
    "\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "\n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        out = self.pipe(prompt, **kwargs)\n",
    "        if isinstance(out, list) and out and isinstance(out[0], dict):\n",
    "            return out[0].get(\"generated_text\") or out[0].get(\"text\") or str(out[0])\n",
    "        return str(out)\n",
    "\n",
    "    def invoke(self, prompt, **kwargs):\n",
    "        kwargs.setdefault(\"max_new_tokens\", 256)\n",
    "        kwargs.setdefault(\"do_sample\", False)\n",
    "        return self.__call__(prompt, **kwargs)\n",
    "\n",
    "    def generate(self, prompt, max_new_tokens=256, **kwargs):\n",
    "        kwargs.setdefault(\"max_new_tokens\", max_new_tokens)\n",
    "        kwargs.setdefault(\"do_sample\", False)\n",
    "        return self.__call__(prompt, **kwargs)\n",
    "\n",
    "llm = HFAdapter(pipe)\n",
    "print(f\"HFAdapter created with text-generation pipeline on device={DEVICE}; llm is ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LangChain HuggingFacePipeline LLM (lc_llm) from text-generation pipeline\n"
     ]
    }
   ],
   "source": [
    "# Create a LangChain-compatible LLM using the existing transformers pipeline\n",
    "# This must wrap a supported pipeline task (text-generation / text2text-generation / summarization / translation).\n",
    "\n",
    "try:\n",
    "    from langchain import HuggingFacePipeline\n",
    "\n",
    "    # Reuse existing `pipe` created above.\n",
    "    hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    lc_llm = hf_llm\n",
    "    print(\"Created LangChain HuggingFacePipeline LLM (lc_llm) from text-generation pipeline\")\n",
    "except Exception as e:\n",
    "    print(\"Could not create HuggingFacePipeline. Falling back to a minimal runnable wrapper:\", e)\n",
    "\n",
    "    class RunnableHF:\n",
    "        def __init__(self, pipe):\n",
    "            self.pipe = pipe\n",
    "\n",
    "        def __call__(self, prompt, **kwargs):\n",
    "            out = self.pipe(prompt, **kwargs)\n",
    "            if isinstance(out, list) and out and isinstance(out[0], dict):\n",
    "                return out[0].get(\"generated_text\") or out[0].get(\"text\") or str(out[0])\n",
    "            return str(out)\n",
    "\n",
    "        def invoke(self, prompt, **kwargs):\n",
    "            kwargs.setdefault(\"max_new_tokens\", 256)\n",
    "            kwargs.setdefault(\"do_sample\", False)\n",
    "            return self.__call__(prompt, **kwargs)\n",
    "\n",
    "    lc_llm = RunnableHF(pipe)\n",
    "    print(\"Created RunnableHF wrapper (lc_llm)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm=CTransformers(model=\"/Users/parthawgoswami/Library/Caches/llama.cpp/TheBloke_Mistral-7B-Instruct-v0.1-GGUF_mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "#                  model_type=\"mistral-7b\",\n",
    "#                  config={'max_new_tokens':2048,\n",
    "#                          'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalQA chain created as qa\n"
     ]
    }
   ],
   "source": [
    "# Build a RetrievalQA chain using the LangChain-compatible `lc_llm`\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=lc_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")\n",
    "print(\"RetrievalQA chain created as qa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive QA loop (uses the already-created `qa` chain)\n",
    "# Tip: if you just changed the pipeline task/model, restart kernel and run cells up to the `qa = ...` cell.\n",
    "\n",
    "i = 1\n",
    "while i < 3:\n",
    "    user_input = input(\"Input Prompt: \").strip()\n",
    "    if not user_input:\n",
    "        print(\"Empty prompt; try again\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Newer LangChain prefers invoke()\n",
    "        result = qa.invoke({\"query\": user_input})\n",
    "    except Exception:\n",
    "        # Backward-compatible fallback\n",
    "        result = qa({\"query\": user_input})\n",
    "\n",
    "    # `RetrievalQA` typically returns {\"result\": ..., \"source_documents\": ...} when return_source_documents=True\n",
    "    answer = result.get(\"result\") if isinstance(result, dict) else str(result)\n",
    "    print(\"Response:\", answer)\n",
    "\n",
    "    if isinstance(result, dict) and \"source_documents\" in result:\n",
    "        print(\"\\nTop sources:\")\n",
    "        for d in result[\"source_documents\"][:2]:\n",
    "            meta = getattr(d, \"metadata\", {}) or {}\n",
    "            src = meta.get(\"source\") or meta.get(\"file_path\") or \"<unknown>\"\n",
    "            snippet = (getattr(d, \"page_con\n",
    "                               \n",
    "                               tent\", \"\") or \"\")[:200].replace(\"\\n\", \" \")\n",
    "            print(f\"- {src}: {snippet}...\")\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The previous error is nothing but a Keybord Interrupt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since the offline LLM takes a while to load and give us a response we will try to use LLM API from GROQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response for user input query \"I have pain in my head\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respone for the user input query \"who is superman?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like we are good to go and proceed with the next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If needed we can use RAGA to input feedback from user on answers and trail the model further using RAGA, but for now it seems good so we will proceed to the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step would be to put this model into a Flask App with modular coding and pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading schemas...\n",
      "read_schema_csv: parsed 1 tables and 16 attributes from /Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/CARDIOVASCULAR_Schema.csv\n",
      "read_schema_csv: parsed 38 tables and 425 attributes from /Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/OMOP_Schema.csv\n",
      "Running deterministic matching (v2)...\n",
      "\n",
      "Predicted mappings (showing non-empty):\n",
      "  CASE FORM -> person_id\n",
      "  PERSONAL RELATIONSHIP -> note_title\n",
      "  IS FOLLOW UP -> person_id\n",
      "  AGE -> year_of_birth\n",
      "  GENDER -> gender_concept_id\n",
      "  ANCESTRY ORIGIN -> ancestor_concept_id\n",
      "  INSURANCE TYPE -> period_type_concept_id\n",
      "  NOT ADDRESSED SOCIAL DETERMINANTS -> gender_concept_id\n",
      "  DETAILS ABOUT SOCIAL FACTORS CHECKED -> observation_source_value\n",
      "  QUESTIONS -> note_title\n",
      "  DIAGNOSES HISTORY -> drug_source_value\n",
      "  SUBSTANCE USE HISTORY -> person_source_value\n",
      "  DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH -> visit_occurrence_id\n",
      "  DETAILS OF DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH -> gender_concept_id\n",
      "  FAMILY DIED OF HEART PROBLEMS OR UNEXPECTED DEATH BEFORE 50 -> dea\n",
      "  LIKE TO LEARN MORE ABOUT -> gender_concept_id\n",
      "\n",
      "Evaluation:\n",
      "  Ground-truth pairs: 25\n",
      "  Predicted pairs: 16\n",
      "  True positives: 7\n",
      "  Precision: 0.438\n",
      "  Recall:    0.280\n",
      "  F1:        0.341\n",
      "\n",
      "\n",
      "False positives (predicted but not in GT):\n",
      "  ('ANCESTRY ORIGIN', 'ancestor_concept_id')\n",
      "  ('DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH', 'visit_occurrence_id')\n",
      "  ('DETAILS OF DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH', 'gender_concept_id')\n",
      "  ('FAMILY DIED OF HEART PROBLEMS OR UNEXPECTED DEATH BEFORE 50', 'dea')\n",
      "  ('INSURANCE TYPE', 'period_type_concept_id')\n",
      "  ('IS FOLLOW UP', 'person_id')\n",
      "  ('LIKE TO LEARN MORE ABOUT', 'gender_concept_id')\n",
      "  ('NOT ADDRESSED SOCIAL DETERMINANTS', 'gender_concept_id')\n",
      "  ('SUBSTANCE USE HISTORY', 'person_source_value')\n",
      "\n",
      "False negatives (in GT but not predicted):\n",
      "  ('ANCESTRY ORIGIN', 'race_concept_id')\n",
      "  ('ANCESTRY ORIGIN', 'race_source_value')\n",
      "  ('CASE FORM', 'person_source_value')\n",
      "  ('DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH', 'observation_source_value')\n",
      "  ('DETAILS OF DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH', 'observation_source_value')\n",
      "  ('FAMILY DIED OF HEART PROBLEMS OR UNEXPECTED DEATH BEFORE 50', 'observation_source_value')\n",
      "  ('GENDER', 'gender_source_value')\n",
      "  ('INSURANCE TYPE', 'note_text')\n",
      "  ('INSURANCE TYPE', 'note_title')\n",
      "  ('IS FOLLOW UP', 'note_text')\n",
      "  ('IS FOLLOW UP', 'note_title')\n",
      "  ('LIKE TO LEARN MORE ABOUT', 'note_text')\n",
      "  ('LIKE TO LEARN MORE ABOUT', 'note_title')\n",
      "  ('NOT ADDRESSED SOCIAL DETERMINANTS', 'observation_source_value')\n",
      "  ('PERSONAL RELATIONSHIP', 'note_text')\n",
      "  ('QUESTIONS', 'note_text')\n",
      "  ('SUBSTANCE USE HISTORY', 'note_text')\n",
      "  ('SUBSTANCE USE HISTORY', 'note_title')\n",
      "\n",
      "Running RAG + LLM matcher (with few-shot and alias seeds but NO ground-truth injection)...\n",
      "\n",
      "Predicted mappings (showing non-empty):\n",
      "  CASE FORM -> person_id\n",
      "  PERSONAL RELATIONSHIP -> note_title\n",
      "  IS FOLLOW UP -> person_id\n",
      "  AGE -> year_of_birth\n",
      "  GENDER -> gender_concept_id\n",
      "  ANCESTRY ORIGIN -> ancestor_concept_id\n",
      "  INSURANCE TYPE -> period_type_concept_id\n",
      "  NOT ADDRESSED SOCIAL DETERMINANTS -> gender_concept_id\n",
      "  DETAILS ABOUT SOCIAL FACTORS CHECKED -> observation_source_value\n",
      "  QUESTIONS -> note_title\n",
      "  DIAGNOSES HISTORY -> drug_source_value\n",
      "  SUBSTANCE USE HISTORY -> person_source_value\n",
      "  DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH -> visit_occurrence_id\n",
      "  DETAILS OF DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH -> gender_concept_id\n",
      "  FAMILY DIED OF HEART PROBLEMS OR UNEXPECTED DEATH BEFORE 50 -> dea\n",
      "  LIKE TO LEARN MORE ABOUT -> gender_concept_id\n",
      "\n",
      "Evaluation:\n",
      "  Ground-truth pairs: 25\n",
      "  Predicted pairs: 16\n",
      "  True positives: 7\n",
      "  Precision: 0.438\n",
      "  Recall:    0.280\n",
      "  F1:        0.341\n",
      "\n",
      "\n",
      "False positives (predicted but not in GT):\n",
      "  ('ANCESTRY ORIGIN', 'ancestor_concept_id')\n",
      "  ('DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH', 'visit_occurrence_id')\n",
      "  ('DETAILS OF DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH', 'gender_concept_id')\n",
      "  ('FAMILY DIED OF HEART PROBLEMS OR UNEXPECTED DEATH BEFORE 50', 'dea')\n",
      "  ('INSURANCE TYPE', 'period_type_concept_id')\n",
      "  ('IS FOLLOW UP', 'person_id')\n",
      "  ('LIKE TO LEARN MORE ABOUT', 'gender_concept_id')\n",
      "  ('NOT ADDRESSED SOCIAL DETERMINANTS', 'gender_concept_id')\n",
      "  ('SUBSTANCE USE HISTORY', 'person_source_value')\n",
      "\n",
      "False negatives (in GT but not predicted):\n",
      "  ('ANCESTRY ORIGIN', 'race_concept_id')\n",
      "  ('ANCESTRY ORIGIN', 'race_source_value')\n",
      "  ('CASE FORM', 'person_source_value')\n",
      "  ('DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH', 'observation_source_value')\n",
      "  ('DETAILS OF DEPRESSION ANXIETY EATING DISORDER MENTAL HEALTH', 'observation_source_value')\n",
      "  ('FAMILY DIED OF HEART PROBLEMS OR UNEXPECTED DEATH BEFORE 50', 'observation_source_value')\n",
      "  ('GENDER', 'gender_source_value')\n",
      "  ('INSURANCE TYPE', 'note_text')\n",
      "  ('INSURANCE TYPE', 'note_title')\n",
      "  ('IS FOLLOW UP', 'note_text')\n",
      "  ('IS FOLLOW UP', 'note_title')\n",
      "  ('LIKE TO LEARN MORE ABOUT', 'note_text')\n",
      "  ('LIKE TO LEARN MORE ABOUT', 'note_title')\n",
      "  ('NOT ADDRESSED SOCIAL DETERMINANTS', 'observation_source_value')\n",
      "  ('PERSONAL RELATIONSHIP', 'note_text')\n",
      "  ('QUESTIONS', 'note_text')\n",
      "  ('SUBSTANCE USE HISTORY', 'note_text')\n",
      "  ('SUBSTANCE USE HISTORY', 'note_title')\n",
      "\n",
      "Running RAG + LLM matcher (with few-shot and alias seeds but NO ground-truth injection)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1432) exceeded maximum context length (512).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded target attributes using HuggingFaceEmbeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1433) exceeded maximum context length (512).\n",
      "Number of tokens (1434) exceeded maximum context length (512).\n",
      "Number of tokens (1434) exceeded maximum context length (512).\n",
      "Number of tokens (1435) exceeded maximum context length (512).\n",
      "Number of tokens (1435) exceeded maximum context length (512).\n",
      "Number of tokens (1436) exceeded maximum context length (512).\n",
      "Number of tokens (1436) exceeded maximum context length (512).\n",
      "Number of tokens (1437) exceeded maximum context length (512).\n",
      "Number of tokens (1437) exceeded maximum context length (512).\n",
      "Number of tokens (1438) exceeded maximum context length (512).\n",
      "Number of tokens (1438) exceeded maximum context length (512).\n",
      "Number of tokens (1439) exceeded maximum context length (512).\n",
      "Number of tokens (1439) exceeded maximum context length (512).\n",
      "Number of tokens (1440) exceeded maximum context length (512).\n",
      "Number of tokens (1440) exceeded maximum context length (512).\n",
      "Number of tokens (1441) exceeded maximum context length (512).\n",
      "Number of tokens (1441) exceeded maximum context length (512).\n",
      "Number of tokens (1442) exceeded maximum context length (512).\n",
      "Number of tokens (1442) exceeded maximum context length (512).\n",
      "Number of tokens (1443) exceeded maximum context length (512).\n",
      "Number of tokens (1443) exceeded maximum context length (512).\n",
      "Number of tokens (1444) exceeded maximum context length (512).\n",
      "Number of tokens (1444) exceeded maximum context length (512).\n",
      "Number of tokens (1445) exceeded maximum context length (512).\n",
      "Number of tokens (1445) exceeded maximum context length (512).\n",
      "Number of tokens (1446) exceeded maximum context length (512).\n",
      "Number of tokens (1446) exceeded maximum context length (512).\n",
      "Number of tokens (1447) exceeded maximum context length (512).\n",
      "Number of tokens (1447) exceeded maximum context length (512).\n",
      "Number of tokens (1448) exceeded maximum context length (512).\n",
      "Number of tokens (1448) exceeded maximum context length (512).\n",
      "Number of tokens (1449) exceeded maximum context length (512).\n",
      "Number of tokens (1449) exceeded maximum context length (512).\n",
      "Number of tokens (1450) exceeded maximum context length (512).\n",
      "Number of tokens (1450) exceeded maximum context length (512).\n",
      "Number of tokens (1451) exceeded maximum context length (512).\n",
      "Number of tokens (1451) exceeded maximum context length (512).\n",
      "Number of tokens (1452) exceeded maximum context length (512).\n",
      "Number of tokens (1452) exceeded maximum context length (512).\n",
      "Number of tokens (1453) exceeded maximum context length (512).\n",
      "Number of tokens (1453) exceeded maximum context length (512).\n",
      "Number of tokens (1454) exceeded maximum context length (512).\n",
      "Number of tokens (1454) exceeded maximum context length (512).\n",
      "Number of tokens (1455) exceeded maximum context length (512).\n",
      "Number of tokens (1455) exceeded maximum context length (512).\n",
      "Number of tokens (1456) exceeded maximum context length (512).\n",
      "Number of tokens (1456) exceeded maximum context length (512).\n",
      "Number of tokens (1457) exceeded maximum context length (512).\n",
      "Number of tokens (1457) exceeded maximum context length (512).\n",
      "Number of tokens (1458) exceeded maximum context length (512).\n",
      "Number of tokens (1458) exceeded maximum context length (512).\n",
      "Number of tokens (1459) exceeded maximum context length (512).\n",
      "Number of tokens (1459) exceeded maximum context length (512).\n",
      "Number of tokens (1460) exceeded maximum context length (512).\n",
      "Number of tokens (1460) exceeded maximum context length (512).\n",
      "Number of tokens (1461) exceeded maximum context length (512).\n",
      "Number of tokens (1461) exceeded maximum context length (512).\n",
      "Number of tokens (1462) exceeded maximum context length (512).\n",
      "Number of tokens (1462) exceeded maximum context length (512).\n",
      "Number of tokens (1463) exceeded maximum context length (512).\n",
      "Number of tokens (1463) exceeded maximum context length (512).\n",
      "Number of tokens (1464) exceeded maximum context length (512).\n",
      "Number of tokens (1464) exceeded maximum context length (512).\n",
      "Number of tokens (1465) exceeded maximum context length (512).\n",
      "Number of tokens (1465) exceeded maximum context length (512).\n",
      "Number of tokens (1466) exceeded maximum context length (512).\n",
      "Number of tokens (1466) exceeded maximum context length (512).\n",
      "Number of tokens (1467) exceeded maximum context length (512).\n",
      "Number of tokens (1467) exceeded maximum context length (512).\n",
      "Number of tokens (1468) exceeded maximum context length (512).\n",
      "Number of tokens (1468) exceeded maximum context length (512).\n",
      "Number of tokens (1469) exceeded maximum context length (512).\n",
      "Number of tokens (1469) exceeded maximum context length (512).\n",
      "Number of tokens (1470) exceeded maximum context length (512).\n",
      "Number of tokens (1470) exceeded maximum context length (512).\n",
      "Number of tokens (1471) exceeded maximum context length (512).\n",
      "Number of tokens (1471) exceeded maximum context length (512).\n",
      "Number of tokens (1472) exceeded maximum context length (512).\n",
      "Number of tokens (1472) exceeded maximum context length (512).\n",
      "Number of tokens (1473) exceeded maximum context length (512).\n",
      "Number of tokens (1473) exceeded maximum context length (512).\n",
      "Number of tokens (1474) exceeded maximum context length (512).\n",
      "Number of tokens (1474) exceeded maximum context length (512).\n",
      "Number of tokens (1475) exceeded maximum context length (512).\n",
      "Number of tokens (1475) exceeded maximum context length (512).\n",
      "Number of tokens (1476) exceeded maximum context length (512).\n",
      "Number of tokens (1476) exceeded maximum context length (512).\n",
      "Number of tokens (1477) exceeded maximum context length (512).\n",
      "Number of tokens (1477) exceeded maximum context length (512).\n",
      "Number of tokens (1478) exceeded maximum context length (512).\n",
      "Number of tokens (1478) exceeded maximum context length (512).\n",
      "Number of tokens (1479) exceeded maximum context length (512).\n",
      "Number of tokens (1479) exceeded maximum context length (512).\n",
      "Number of tokens (1480) exceeded maximum context length (512).\n",
      "Number of tokens (1480) exceeded maximum context length (512).\n",
      "Number of tokens (1481) exceeded maximum context length (512).\n",
      "Number of tokens (1481) exceeded maximum context length (512).\n",
      "Number of tokens (1482) exceeded maximum context length (512).\n",
      "Number of tokens (1482) exceeded maximum context length (512).\n",
      "Number of tokens (1483) exceeded maximum context length (512).\n",
      "Number of tokens (1483) exceeded maximum context length (512).\n",
      "Number of tokens (1484) exceeded maximum context length (512).\n",
      "Number of tokens (1484) exceeded maximum context length (512).\n",
      "Number of tokens (1485) exceeded maximum context length (512).\n",
      "Number of tokens (1485) exceeded maximum context length (512).\n",
      "Number of tokens (1486) exceeded maximum context length (512).\n",
      "Number of tokens (1486) exceeded maximum context length (512).\n",
      "Number of tokens (1487) exceeded maximum context length (512).\n",
      "Number of tokens (1487) exceeded maximum context length (512).\n",
      "Number of tokens (1488) exceeded maximum context length (512).\n",
      "Number of tokens (1488) exceeded maximum context length (512).\n",
      "Number of tokens (1489) exceeded maximum context length (512).\n",
      "Number of tokens (1489) exceeded maximum context length (512).\n",
      "Number of tokens (1490) exceeded maximum context length (512).\n",
      "Number of tokens (1490) exceeded maximum context length (512).\n",
      "Number of tokens (1491) exceeded maximum context length (512).\n",
      "Number of tokens (1491) exceeded maximum context length (512).\n",
      "Number of tokens (1492) exceeded maximum context length (512).\n",
      "Number of tokens (1492) exceeded maximum context length (512).\n",
      "Number of tokens (1493) exceeded maximum context length (512).\n",
      "Number of tokens (1493) exceeded maximum context length (512).\n",
      "Number of tokens (1494) exceeded maximum context length (512).\n",
      "Number of tokens (1494) exceeded maximum context length (512).\n",
      "Number of tokens (1495) exceeded maximum context length (512).\n",
      "Number of tokens (1495) exceeded maximum context length (512).\n",
      "Number of tokens (1496) exceeded maximum context length (512).\n",
      "Number of tokens (1496) exceeded maximum context length (512).\n",
      "Number of tokens (1497) exceeded maximum context length (512).\n",
      "Number of tokens (1497) exceeded maximum context length (512).\n",
      "Number of tokens (1498) exceeded maximum context length (512).\n",
      "Number of tokens (1498) exceeded maximum context length (512).\n",
      "Number of tokens (1499) exceeded maximum context length (512).\n",
      "Number of tokens (1499) exceeded maximum context length (512).\n",
      "Number of tokens (1500) exceeded maximum context length (512).\n",
      "Number of tokens (1500) exceeded maximum context length (512).\n",
      "Number of tokens (1501) exceeded maximum context length (512).\n",
      "Number of tokens (1501) exceeded maximum context length (512).\n",
      "Number of tokens (1502) exceeded maximum context length (512).\n",
      "Number of tokens (1502) exceeded maximum context length (512).\n",
      "Number of tokens (1503) exceeded maximum context length (512).\n",
      "Number of tokens (1503) exceeded maximum context length (512).\n",
      "Number of tokens (1504) exceeded maximum context length (512).\n",
      "Number of tokens (1504) exceeded maximum context length (512).\n",
      "Number of tokens (1505) exceeded maximum context length (512).\n",
      "Number of tokens (1505) exceeded maximum context length (512).\n",
      "Number of tokens (1506) exceeded maximum context length (512).\n",
      "Number of tokens (1506) exceeded maximum context length (512).\n",
      "Number of tokens (1507) exceeded maximum context length (512).\n",
      "Number of tokens (1507) exceeded maximum context length (512).\n",
      "Number of tokens (1508) exceeded maximum context length (512).\n",
      "Number of tokens (1508) exceeded maximum context length (512).\n",
      "Number of tokens (1509) exceeded maximum context length (512).\n",
      "Number of tokens (1509) exceeded maximum context length (512).\n",
      "Number of tokens (1510) exceeded maximum context length (512).\n",
      "Number of tokens (1510) exceeded maximum context length (512).\n",
      "Number of tokens (1511) exceeded maximum context length (512).\n",
      "Number of tokens (1511) exceeded maximum context length (512).\n",
      "Number of tokens (1512) exceeded maximum context length (512).\n",
      "Number of tokens (1512) exceeded maximum context length (512).\n",
      "Number of tokens (1513) exceeded maximum context length (512).\n",
      "Number of tokens (1513) exceeded maximum context length (512).\n",
      "Number of tokens (1514) exceeded maximum context length (512).\n",
      "Number of tokens (1514) exceeded maximum context length (512).\n",
      "Number of tokens (1515) exceeded maximum context length (512).\n",
      "Number of tokens (1515) exceeded maximum context length (512).\n",
      "Number of tokens (1516) exceeded maximum context length (512).\n",
      "Number of tokens (1516) exceeded maximum context length (512).\n",
      "Number of tokens (1517) exceeded maximum context length (512).\n",
      "Number of tokens (1517) exceeded maximum context length (512).\n",
      "Number of tokens (1518) exceeded maximum context length (512).\n",
      "Number of tokens (1518) exceeded maximum context length (512).\n",
      "Number of tokens (1519) exceeded maximum context length (512).\n",
      "Number of tokens (1519) exceeded maximum context length (512).\n",
      "Number of tokens (1520) exceeded maximum context length (512).\n",
      "Number of tokens (1520) exceeded maximum context length (512).\n",
      "Number of tokens (1521) exceeded maximum context length (512).\n",
      "Number of tokens (1521) exceeded maximum context length (512).\n",
      "Number of tokens (1522) exceeded maximum context length (512).\n",
      "Number of tokens (1522) exceeded maximum context length (512).\n",
      "Number of tokens (1523) exceeded maximum context length (512).\n",
      "Number of tokens (1523) exceeded maximum context length (512).\n",
      "Number of tokens (1524) exceeded maximum context length (512).\n",
      "Number of tokens (1524) exceeded maximum context length (512).\n",
      "Number of tokens (1525) exceeded maximum context length (512).\n",
      "Number of tokens (1525) exceeded maximum context length (512).\n",
      "Number of tokens (1526) exceeded maximum context length (512).\n",
      "Number of tokens (1526) exceeded maximum context length (512).\n",
      "Number of tokens (1527) exceeded maximum context length (512).\n",
      "Number of tokens (1527) exceeded maximum context length (512).\n",
      "Number of tokens (1528) exceeded maximum context length (512).\n",
      "Number of tokens (1528) exceeded maximum context length (512).\n",
      "Number of tokens (1529) exceeded maximum context length (512).\n",
      "Number of tokens (1529) exceeded maximum context length (512).\n",
      "Number of tokens (1530) exceeded maximum context length (512).\n",
      "Number of tokens (1530) exceeded maximum context length (512).\n",
      "Number of tokens (1531) exceeded maximum context length (512).\n",
      "Number of tokens (1531) exceeded maximum context length (512).\n",
      "Number of tokens (1532) exceeded maximum context length (512).\n",
      "Number of tokens (1532) exceeded maximum context length (512).\n",
      "Number of tokens (1533) exceeded maximum context length (512).\n",
      "Number of tokens (1533) exceeded maximum context length (512).\n",
      "Number of tokens (1534) exceeded maximum context length (512).\n",
      "Number of tokens (1534) exceeded maximum context length (512).\n",
      "Number of tokens (1535) exceeded maximum context length (512).\n",
      "Number of tokens (1535) exceeded maximum context length (512).\n",
      "Number of tokens (1536) exceeded maximum context length (512).\n",
      "Number of tokens (1536) exceeded maximum context length (512).\n",
      "Number of tokens (1537) exceeded maximum context length (512).\n",
      "Number of tokens (1537) exceeded maximum context length (512).\n",
      "Number of tokens (1538) exceeded maximum context length (512).\n",
      "Number of tokens (1538) exceeded maximum context length (512).\n",
      "Number of tokens (1539) exceeded maximum context length (512).\n",
      "Number of tokens (1539) exceeded maximum context length (512).\n",
      "Number of tokens (1540) exceeded maximum context length (512).\n",
      "Number of tokens (1540) exceeded maximum context length (512).\n",
      "Number of tokens (1541) exceeded maximum context length (512).\n",
      "Number of tokens (1541) exceeded maximum context length (512).\n",
      "Number of tokens (1542) exceeded maximum context length (512).\n",
      "Number of tokens (1542) exceeded maximum context length (512).\n",
      "Number of tokens (1543) exceeded maximum context length (512).\n",
      "Number of tokens (1543) exceeded maximum context length (512).\n",
      "Number of tokens (1544) exceeded maximum context length (512).\n",
      "Number of tokens (1544) exceeded maximum context length (512).\n",
      "Number of tokens (1545) exceeded maximum context length (512).\n",
      "Number of tokens (1545) exceeded maximum context length (512).\n",
      "Number of tokens (1546) exceeded maximum context length (512).\n",
      "Number of tokens (1546) exceeded maximum context length (512).\n",
      "Number of tokens (1547) exceeded maximum context length (512).\n",
      "Number of tokens (1547) exceeded maximum context length (512).\n",
      "Number of tokens (1548) exceeded maximum context length (512).\n",
      "Number of tokens (1548) exceeded maximum context length (512).\n",
      "Number of tokens (1549) exceeded maximum context length (512).\n",
      "Number of tokens (1549) exceeded maximum context length (512).\n",
      "Number of tokens (1550) exceeded maximum context length (512).\n",
      "Number of tokens (1550) exceeded maximum context length (512).\n",
      "Number of tokens (1551) exceeded maximum context length (512).\n",
      "Number of tokens (1551) exceeded maximum context length (512).\n",
      "Number of tokens (1552) exceeded maximum context length (512).\n",
      "Number of tokens (1552) exceeded maximum context length (512).\n",
      "Number of tokens (1553) exceeded maximum context length (512).\n",
      "Number of tokens (1553) exceeded maximum context length (512).\n",
      "Number of tokens (1554) exceeded maximum context length (512).\n",
      "Number of tokens (1554) exceeded maximum context length (512).\n",
      "Number of tokens (1555) exceeded maximum context length (512).\n",
      "Number of tokens (1555) exceeded maximum context length (512).\n",
      "Number of tokens (1556) exceeded maximum context length (512).\n",
      "Number of tokens (1556) exceeded maximum context length (512).\n",
      "Number of tokens (1557) exceeded maximum context length (512).\n",
      "Number of tokens (1557) exceeded maximum context length (512).\n",
      "Number of tokens (1558) exceeded maximum context length (512).\n",
      "Number of tokens (1558) exceeded maximum context length (512).\n",
      "Number of tokens (1559) exceeded maximum context length (512).\n",
      "Number of tokens (1559) exceeded maximum context length (512).\n",
      "Number of tokens (1560) exceeded maximum context length (512).\n",
      "Number of tokens (1560) exceeded maximum context length (512).\n",
      "Number of tokens (1561) exceeded maximum context length (512).\n",
      "Number of tokens (1561) exceeded maximum context length (512).\n",
      "Number of tokens (1562) exceeded maximum context length (512).\n",
      "Number of tokens (1562) exceeded maximum context length (512).\n",
      "Number of tokens (1563) exceeded maximum context length (512).\n",
      "Number of tokens (1563) exceeded maximum context length (512).\n",
      "Number of tokens (1564) exceeded maximum context length (512).\n",
      "Number of tokens (1564) exceeded maximum context length (512).\n",
      "Number of tokens (1565) exceeded maximum context length (512).\n",
      "Number of tokens (1565) exceeded maximum context length (512).\n",
      "Number of tokens (1566) exceeded maximum context length (512).\n",
      "Number of tokens (1566) exceeded maximum context length (512).\n",
      "Number of tokens (1567) exceeded maximum context length (512).\n",
      "Number of tokens (1567) exceeded maximum context length (512).\n",
      "Number of tokens (1568) exceeded maximum context length (512).\n",
      "Number of tokens (1568) exceeded maximum context length (512).\n",
      "Number of tokens (1569) exceeded maximum context length (512).\n",
      "Number of tokens (1569) exceeded maximum context length (512).\n",
      "Number of tokens (1570) exceeded maximum context length (512).\n",
      "Number of tokens (1570) exceeded maximum context length (512).\n",
      "Number of tokens (1571) exceeded maximum context length (512).\n",
      "Number of tokens (1571) exceeded maximum context length (512).\n",
      "Number of tokens (1572) exceeded maximum context length (512).\n",
      "Number of tokens (1572) exceeded maximum context length (512).\n",
      "Number of tokens (1573) exceeded maximum context length (512).\n",
      "Number of tokens (1573) exceeded maximum context length (512).\n",
      "Number of tokens (1574) exceeded maximum context length (512).\n",
      "Number of tokens (1574) exceeded maximum context length (512).\n",
      "Number of tokens (1575) exceeded maximum context length (512).\n",
      "Number of tokens (1575) exceeded maximum context length (512).\n",
      "Number of tokens (1576) exceeded maximum context length (512).\n",
      "Number of tokens (1576) exceeded maximum context length (512).\n",
      "Number of tokens (1577) exceeded maximum context length (512).\n",
      "Number of tokens (1577) exceeded maximum context length (512).\n",
      "Number of tokens (1578) exceeded maximum context length (512).\n",
      "Number of tokens (1578) exceeded maximum context length (512).\n",
      "Number of tokens (1579) exceeded maximum context length (512).\n",
      "Number of tokens (1579) exceeded maximum context length (512).\n",
      "Number of tokens (1580) exceeded maximum context length (512).\n",
      "Number of tokens (1580) exceeded maximum context length (512).\n",
      "Number of tokens (1581) exceeded maximum context length (512).\n",
      "Number of tokens (1581) exceeded maximum context length (512).\n",
      "Number of tokens (1582) exceeded maximum context length (512).\n",
      "Number of tokens (1582) exceeded maximum context length (512).\n",
      "Number of tokens (1583) exceeded maximum context length (512).\n",
      "Number of tokens (1583) exceeded maximum context length (512).\n",
      "Number of tokens (1584) exceeded maximum context length (512).\n",
      "Number of tokens (1584) exceeded maximum context length (512).\n",
      "Number of tokens (1585) exceeded maximum context length (512).\n",
      "Number of tokens (1585) exceeded maximum context length (512).\n",
      "Number of tokens (1586) exceeded maximum context length (512).\n",
      "Number of tokens (1586) exceeded maximum context length (512).\n",
      "Number of tokens (1587) exceeded maximum context length (512).\n",
      "Number of tokens (1587) exceeded maximum context length (512).\n",
      "Number of tokens (1588) exceeded maximum context length (512).\n",
      "Number of tokens (1588) exceeded maximum context length (512).\n",
      "Number of tokens (1589) exceeded maximum context length (512).\n",
      "Number of tokens (1589) exceeded maximum context length (512).\n",
      "Number of tokens (1590) exceeded maximum context length (512).\n",
      "Number of tokens (1590) exceeded maximum context length (512).\n",
      "Number of tokens (1591) exceeded maximum context length (512).\n",
      "Number of tokens (1591) exceeded maximum context length (512).\n",
      "Number of tokens (1592) exceeded maximum context length (512).\n",
      "Number of tokens (1592) exceeded maximum context length (512).\n",
      "Number of tokens (1593) exceeded maximum context length (512).\n",
      "Number of tokens (1593) exceeded maximum context length (512).\n",
      "Number of tokens (1594) exceeded maximum context length (512).\n",
      "Number of tokens (1594) exceeded maximum context length (512).\n",
      "Number of tokens (1595) exceeded maximum context length (512).\n",
      "Number of tokens (1595) exceeded maximum context length (512).\n",
      "Number of tokens (1596) exceeded maximum context length (512).\n",
      "Number of tokens (1596) exceeded maximum context length (512).\n",
      "Number of tokens (1597) exceeded maximum context length (512).\n",
      "Number of tokens (1597) exceeded maximum context length (512).\n",
      "Number of tokens (1598) exceeded maximum context length (512).\n",
      "Number of tokens (1598) exceeded maximum context length (512).\n",
      "Number of tokens (1599) exceeded maximum context length (512).\n",
      "Number of tokens (1599) exceeded maximum context length (512).\n",
      "Number of tokens (1600) exceeded maximum context length (512).\n",
      "Number of tokens (1600) exceeded maximum context length (512).\n",
      "Number of tokens (1601) exceeded maximum context length (512).\n",
      "Number of tokens (1601) exceeded maximum context length (512).\n",
      "Number of tokens (1602) exceeded maximum context length (512).\n",
      "Number of tokens (1602) exceeded maximum context length (512).\n",
      "Number of tokens (1603) exceeded maximum context length (512).\n",
      "Number of tokens (1603) exceeded maximum context length (512).\n",
      "Number of tokens (1604) exceeded maximum context length (512).\n",
      "Number of tokens (1604) exceeded maximum context length (512).\n",
      "Number of tokens (1605) exceeded maximum context length (512).\n",
      "Number of tokens (1605) exceeded maximum context length (512).\n",
      "Number of tokens (1606) exceeded maximum context length (512).\n",
      "Number of tokens (1606) exceeded maximum context length (512).\n",
      "Number of tokens (1607) exceeded maximum context length (512).\n",
      "Number of tokens (1607) exceeded maximum context length (512).\n",
      "Number of tokens (1608) exceeded maximum context length (512).\n",
      "Number of tokens (1608) exceeded maximum context length (512).\n",
      "Number of tokens (1609) exceeded maximum context length (512).\n",
      "Number of tokens (1609) exceeded maximum context length (512).\n",
      "Number of tokens (1610) exceeded maximum context length (512).\n",
      "Number of tokens (1610) exceeded maximum context length (512).\n",
      "Number of tokens (1611) exceeded maximum context length (512).\n",
      "Number of tokens (1611) exceeded maximum context length (512).\n",
      "Number of tokens (1612) exceeded maximum context length (512).\n",
      "Number of tokens (1612) exceeded maximum context length (512).\n",
      "Number of tokens (1613) exceeded maximum context length (512).\n",
      "Number of tokens (1613) exceeded maximum context length (512).\n",
      "Number of tokens (1614) exceeded maximum context length (512).\n",
      "Number of tokens (1614) exceeded maximum context length (512).\n",
      "Number of tokens (1615) exceeded maximum context length (512).\n",
      "Number of tokens (1615) exceeded maximum context length (512).\n",
      "Number of tokens (1616) exceeded maximum context length (512).\n",
      "Number of tokens (1616) exceeded maximum context length (512).\n",
      "Number of tokens (1617) exceeded maximum context length (512).\n",
      "Number of tokens (1617) exceeded maximum context length (512).\n",
      "Number of tokens (1618) exceeded maximum context length (512).\n",
      "Number of tokens (1618) exceeded maximum context length (512).\n",
      "Number of tokens (1619) exceeded maximum context length (512).\n",
      "Number of tokens (1619) exceeded maximum context length (512).\n",
      "Number of tokens (1620) exceeded maximum context length (512).\n",
      "Number of tokens (1620) exceeded maximum context length (512).\n",
      "Number of tokens (1621) exceeded maximum context length (512).\n",
      "Number of tokens (1621) exceeded maximum context length (512).\n",
      "Number of tokens (1622) exceeded maximum context length (512).\n",
      "Number of tokens (1622) exceeded maximum context length (512).\n",
      "Number of tokens (1623) exceeded maximum context length (512).\n",
      "Number of tokens (1623) exceeded maximum context length (512).\n",
      "Number of tokens (1624) exceeded maximum context length (512).\n",
      "Number of tokens (1624) exceeded maximum context length (512).\n",
      "Number of tokens (1625) exceeded maximum context length (512).\n",
      "Number of tokens (1625) exceeded maximum context length (512).\n",
      "Number of tokens (1626) exceeded maximum context length (512).\n",
      "Number of tokens (1626) exceeded maximum context length (512).\n",
      "Number of tokens (1627) exceeded maximum context length (512).\n",
      "Number of tokens (1627) exceeded maximum context length (512).\n",
      "Number of tokens (1628) exceeded maximum context length (512).\n",
      "Number of tokens (1628) exceeded maximum context length (512).\n",
      "Number of tokens (1629) exceeded maximum context length (512).\n",
      "Number of tokens (1629) exceeded maximum context length (512).\n",
      "Number of tokens (1630) exceeded maximum context length (512).\n",
      "Number of tokens (1630) exceeded maximum context length (512).\n",
      "Number of tokens (1631) exceeded maximum context length (512).\n",
      "Number of tokens (1631) exceeded maximum context length (512).\n",
      "Number of tokens (1632) exceeded maximum context length (512).\n",
      "Number of tokens (1632) exceeded maximum context length (512).\n",
      "Number of tokens (1633) exceeded maximum context length (512).\n",
      "Number of tokens (1633) exceeded maximum context length (512).\n",
      "Number of tokens (1634) exceeded maximum context length (512).\n",
      "Number of tokens (1634) exceeded maximum context length (512).\n",
      "Number of tokens (1635) exceeded maximum context length (512).\n",
      "Number of tokens (1635) exceeded maximum context length (512).\n",
      "Number of tokens (1636) exceeded maximum context length (512).\n",
      "Number of tokens (1636) exceeded maximum context length (512).\n",
      "Number of tokens (1637) exceeded maximum context length (512).\n",
      "Number of tokens (1637) exceeded maximum context length (512).\n",
      "Number of tokens (1638) exceeded maximum context length (512).\n",
      "Number of tokens (1638) exceeded maximum context length (512).\n",
      "Number of tokens (1639) exceeded maximum context length (512).\n",
      "Number of tokens (1639) exceeded maximum context length (512).\n",
      "Number of tokens (1640) exceeded maximum context length (512).\n",
      "Number of tokens (1640) exceeded maximum context length (512).\n",
      "Number of tokens (1641) exceeded maximum context length (512).\n",
      "Number of tokens (1641) exceeded maximum context length (512).\n",
      "Number of tokens (1642) exceeded maximum context length (512).\n",
      "Number of tokens (1642) exceeded maximum context length (512).\n",
      "Number of tokens (1643) exceeded maximum context length (512).\n",
      "Number of tokens (1643) exceeded maximum context length (512).\n",
      "Number of tokens (1644) exceeded maximum context length (512).\n",
      "Number of tokens (1644) exceeded maximum context length (512).\n",
      "Number of tokens (1645) exceeded maximum context length (512).\n",
      "Number of tokens (1645) exceeded maximum context length (512).\n",
      "Number of tokens (1646) exceeded maximum context length (512).\n",
      "Number of tokens (1646) exceeded maximum context length (512).\n",
      "Number of tokens (1647) exceeded maximum context length (512).\n",
      "Number of tokens (1647) exceeded maximum context length (512).\n",
      "Number of tokens (1648) exceeded maximum context length (512).\n",
      "Number of tokens (1648) exceeded maximum context length (512).\n",
      "Number of tokens (1649) exceeded maximum context length (512).\n",
      "Number of tokens (1649) exceeded maximum context length (512).\n",
      "Number of tokens (1650) exceeded maximum context length (512).\n",
      "Number of tokens (1650) exceeded maximum context length (512).\n",
      "Number of tokens (1651) exceeded maximum context length (512).\n",
      "Number of tokens (1651) exceeded maximum context length (512).\n",
      "Number of tokens (1652) exceeded maximum context length (512).\n",
      "Number of tokens (1652) exceeded maximum context length (512).\n",
      "Number of tokens (1653) exceeded maximum context length (512).\n",
      "Number of tokens (1653) exceeded maximum context length (512).\n",
      "Number of tokens (1654) exceeded maximum context length (512).\n",
      "Number of tokens (1654) exceeded maximum context length (512).\n",
      "Number of tokens (1655) exceeded maximum context length (512).\n",
      "Number of tokens (1655) exceeded maximum context length (512).\n",
      "Number of tokens (1656) exceeded maximum context length (512).\n",
      "Number of tokens (1656) exceeded maximum context length (512).\n",
      "Number of tokens (1657) exceeded maximum context length (512).\n",
      "Number of tokens (1657) exceeded maximum context length (512).\n",
      "Number of tokens (1658) exceeded maximum context length (512).\n",
      "Number of tokens (1658) exceeded maximum context length (512).\n",
      "Number of tokens (1659) exceeded maximum context length (512).\n",
      "Number of tokens (1659) exceeded maximum context length (512).\n",
      "Number of tokens (1660) exceeded maximum context length (512).\n",
      "Number of tokens (1660) exceeded maximum context length (512).\n",
      "Number of tokens (1661) exceeded maximum context length (512).\n",
      "Number of tokens (1661) exceeded maximum context length (512).\n",
      "Number of tokens (1662) exceeded maximum context length (512).\n",
      "Number of tokens (1662) exceeded maximum context length (512).\n",
      "Number of tokens (1663) exceeded maximum context length (512).\n",
      "Number of tokens (1663) exceeded maximum context length (512).\n",
      "Number of tokens (1664) exceeded maximum context length (512).\n",
      "Number of tokens (1664) exceeded maximum context length (512).\n",
      "Number of tokens (1665) exceeded maximum context length (512).\n",
      "Number of tokens (1665) exceeded maximum context length (512).\n",
      "Number of tokens (1666) exceeded maximum context length (512).\n",
      "Number of tokens (1666) exceeded maximum context length (512).\n",
      "Number of tokens (1667) exceeded maximum context length (512).\n",
      "Number of tokens (1667) exceeded maximum context length (512).\n",
      "Number of tokens (1668) exceeded maximum context length (512).\n",
      "Number of tokens (1668) exceeded maximum context length (512).\n",
      "Number of tokens (1669) exceeded maximum context length (512).\n",
      "Number of tokens (1669) exceeded maximum context length (512).\n",
      "Number of tokens (1670) exceeded maximum context length (512).\n",
      "Number of tokens (1670) exceeded maximum context length (512).\n",
      "Number of tokens (1671) exceeded maximum context length (512).\n",
      "Number of tokens (1671) exceeded maximum context length (512).\n",
      "Number of tokens (1672) exceeded maximum context length (512).\n",
      "Number of tokens (1672) exceeded maximum context length (512).\n",
      "Number of tokens (1673) exceeded maximum context length (512).\n",
      "Number of tokens (1673) exceeded maximum context length (512).\n",
      "Number of tokens (1674) exceeded maximum context length (512).\n",
      "Number of tokens (1674) exceeded maximum context length (512).\n",
      "Number of tokens (1675) exceeded maximum context length (512).\n",
      "Number of tokens (1675) exceeded maximum context length (512).\n",
      "Number of tokens (1676) exceeded maximum context length (512).\n",
      "Number of tokens (1676) exceeded maximum context length (512).\n",
      "Number of tokens (1677) exceeded maximum context length (512).\n",
      "Number of tokens (1677) exceeded maximum context length (512).\n",
      "Number of tokens (1678) exceeded maximum context length (512).\n",
      "Number of tokens (1678) exceeded maximum context length (512).\n",
      "Number of tokens (1679) exceeded maximum context length (512).\n",
      "Number of tokens (1679) exceeded maximum context length (512).\n",
      "Number of tokens (1680) exceeded maximum context length (512).\n",
      "Number of tokens (1680) exceeded maximum context length (512).\n",
      "Number of tokens (1681) exceeded maximum context length (512).\n",
      "Number of tokens (1681) exceeded maximum context length (512).\n",
      "Number of tokens (1682) exceeded maximum context length (512).\n",
      "Number of tokens (1682) exceeded maximum context length (512).\n",
      "Number of tokens (1683) exceeded maximum context length (512).\n",
      "Number of tokens (1683) exceeded maximum context length (512).\n",
      "Number of tokens (1684) exceeded maximum context length (512).\n",
      "Number of tokens (1684) exceeded maximum context length (512).\n",
      "Number of tokens (1685) exceeded maximum context length (512).\n",
      "Number of tokens (1685) exceeded maximum context length (512).\n",
      "Number of tokens (1686) exceeded maximum context length (512).\n",
      "Number of tokens (1686) exceeded maximum context length (512).\n",
      "Number of tokens (1687) exceeded maximum context length (512).\n",
      "Number of tokens (1687) exceeded maximum context length (512).\n",
      "Number of tokens (1688) exceeded maximum context length (512).\n",
      "Number of tokens (1688) exceeded maximum context length (512).\n",
      "Number of tokens (1689) exceeded maximum context length (512).\n",
      "Number of tokens (1689) exceeded maximum context length (512).\n",
      "Number of tokens (1690) exceeded maximum context length (512).\n",
      "Number of tokens (1690) exceeded maximum context length (512).\n",
      "Number of tokens (1691) exceeded maximum context length (512).\n",
      "Number of tokens (1691) exceeded maximum context length (512).\n",
      "Number of tokens (1692) exceeded maximum context length (512).\n",
      "Number of tokens (1692) exceeded maximum context length (512).\n",
      "Number of tokens (1693) exceeded maximum context length (512).\n",
      "Number of tokens (1693) exceeded maximum context length (512).\n",
      "Number of tokens (1694) exceeded maximum context length (512).\n",
      "Number of tokens (1694) exceeded maximum context length (512).\n",
      "Number of tokens (1695) exceeded maximum context length (512).\n",
      "Number of tokens (1695) exceeded maximum context length (512).\n",
      "Number of tokens (1696) exceeded maximum context length (512).\n",
      "Number of tokens (1696) exceeded maximum context length (512).\n",
      "Number of tokens (1697) exceeded maximum context length (512).\n",
      "Number of tokens (1697) exceeded maximum context length (512).\n",
      "Number of tokens (1698) exceeded maximum context length (512).\n",
      "Number of tokens (1698) exceeded maximum context length (512).\n",
      "Number of tokens (1699) exceeded maximum context length (512).\n",
      "Number of tokens (1699) exceeded maximum context length (512).\n",
      "Number of tokens (1700) exceeded maximum context length (512).\n",
      "Number of tokens (1700) exceeded maximum context length (512).\n",
      "Number of tokens (1701) exceeded maximum context length (512).\n",
      "Number of tokens (1701) exceeded maximum context length (512).\n",
      "Number of tokens (1702) exceeded maximum context length (512).\n",
      "Number of tokens (1702) exceeded maximum context length (512).\n",
      "Number of tokens (1703) exceeded maximum context length (512).\n",
      "Number of tokens (1703) exceeded maximum context length (512).\n",
      "Number of tokens (1704) exceeded maximum context length (512).\n",
      "Number of tokens (1704) exceeded maximum context length (512).\n",
      "Number of tokens (1705) exceeded maximum context length (512).\n",
      "Number of tokens (1705) exceeded maximum context length (512).\n",
      "Number of tokens (1706) exceeded maximum context length (512).\n",
      "Number of tokens (1706) exceeded maximum context length (512).\n",
      "Number of tokens (1707) exceeded maximum context length (512).\n",
      "Number of tokens (1707) exceeded maximum context length (512).\n",
      "Number of tokens (1708) exceeded maximum context length (512).\n",
      "Number of tokens (1708) exceeded maximum context length (512).\n",
      "Number of tokens (1709) exceeded maximum context length (512).\n",
      "Number of tokens (1709) exceeded maximum context length (512).\n",
      "Number of tokens (1710) exceeded maximum context length (512).\n",
      "Number of tokens (1710) exceeded maximum context length (512).\n",
      "Number of tokens (1711) exceeded maximum context length (512).\n",
      "Number of tokens (1711) exceeded maximum context length (512).\n",
      "Number of tokens (1712) exceeded maximum context length (512).\n",
      "Number of tokens (1712) exceeded maximum context length (512).\n",
      "Number of tokens (1713) exceeded maximum context length (512).\n",
      "Number of tokens (1713) exceeded maximum context length (512).\n",
      "Number of tokens (1714) exceeded maximum context length (512).\n",
      "Number of tokens (1714) exceeded maximum context length (512).\n",
      "Number of tokens (1715) exceeded maximum context length (512).\n",
      "Number of tokens (1715) exceeded maximum context length (512).\n",
      "Number of tokens (1716) exceeded maximum context length (512).\n",
      "Number of tokens (1716) exceeded maximum context length (512).\n",
      "Number of tokens (1717) exceeded maximum context length (512).\n",
      "Number of tokens (1717) exceeded maximum context length (512).\n",
      "Number of tokens (1718) exceeded maximum context length (512).\n",
      "Number of tokens (1718) exceeded maximum context length (512).\n",
      "Number of tokens (1719) exceeded maximum context length (512).\n",
      "Number of tokens (1719) exceeded maximum context length (512).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LLM candidate] Source: IS FOLLOW UP -> Option  Option  Table============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================== CONSTAGE |\n",
      "[LLM candidate parsed] ['NONE']\n",
      "\n",
      "[LLM refiner] Source: IS FOLLOW UP -> ```\n",
      "[LLM refiner parsed] ['condition_status_source_value', 'condition_source_value', 'visit_detail_source_value', 'visit_source_value', 'admitted_from_source_value', 'respondent_type_source_value', 'term_exists', 'disease_status_source_value', 'term_modifiers', 'survey_version_number']\n",
      "\n",
      "[LLM refiner] Source: IS FOLLOW UP -> ```\n",
      "[LLM refiner parsed] ['condition_status_source_value', 'condition_source_value', 'visit_detail_source_value', 'visit_source_value', 'admitted_from_source_value', 'respondent_type_source_value', 'term_exists', 'disease_status_source_value', 'term_modifiers', 'survey_version_number']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1515) exceeded maximum context length (512).\n",
      "Number of tokens (1516) exceeded maximum context length (512).\n",
      "Number of tokens (1516) exceeded maximum context length (512).\n",
      "Number of tokens (1517) exceeded maximum context length (512).\n",
      "Number of tokens (1517) exceeded maximum context length (512).\n",
      "Number of tokens (1518) exceeded maximum context length (512).\n",
      "Number of tokens (1518) exceeded maximum context length (512).\n",
      "Number of tokens (1519) exceeded maximum context length (512).\n",
      "Number of tokens (1519) exceeded maximum context length (512).\n",
      "Number of tokens (1520) exceeded maximum context length (512).\n",
      "Number of tokens (1520) exceeded maximum context length (512).\n",
      "Number of tokens (1521) exceeded maximum context length (512).\n",
      "Number of tokens (1521) exceeded maximum context length (512).\n",
      "Number of tokens (1522) exceeded maximum context length (512).\n",
      "Number of tokens (1522) exceeded maximum context length (512).\n",
      "Number of tokens (1523) exceeded maximum context length (512).\n",
      "Number of tokens (1523) exceeded maximum context length (512).\n",
      "Number of tokens (1524) exceeded maximum context length (512).\n",
      "Number of tokens (1524) exceeded maximum context length (512).\n",
      "Number of tokens (1525) exceeded maximum context length (512).\n",
      "Number of tokens (1525) exceeded maximum context length (512).\n",
      "Number of tokens (1526) exceeded maximum context length (512).\n",
      "Number of tokens (1526) exceeded maximum context length (512).\n",
      "Number of tokens (1527) exceeded maximum context length (512).\n",
      "Number of tokens (1527) exceeded maximum context length (512).\n",
      "Number of tokens (1528) exceeded maximum context length (512).\n",
      "Number of tokens (1528) exceeded maximum context length (512).\n",
      "Number of tokens (1529) exceeded maximum context length (512).\n",
      "Number of tokens (1529) exceeded maximum context length (512).\n",
      "Number of tokens (1530) exceeded maximum context length (512).\n",
      "Number of tokens (1530) exceeded maximum context length (512).\n",
      "Number of tokens (1531) exceeded maximum context length (512).\n",
      "Number of tokens (1531) exceeded maximum context length (512).\n",
      "Number of tokens (1532) exceeded maximum context length (512).\n",
      "Number of tokens (1532) exceeded maximum context length (512).\n",
      "Number of tokens (1533) exceeded maximum context length (512).\n",
      "Number of tokens (1533) exceeded maximum context length (512).\n",
      "Number of tokens (1534) exceeded maximum context length (512).\n",
      "Number of tokens (1534) exceeded maximum context length (512).\n",
      "Number of tokens (1535) exceeded maximum context length (512).\n",
      "Number of tokens (1535) exceeded maximum context length (512).\n",
      "Number of tokens (1536) exceeded maximum context length (512).\n",
      "Number of tokens (1536) exceeded maximum context length (512).\n",
      "Number of tokens (1537) exceeded maximum context length (512).\n",
      "Number of tokens (1537) exceeded maximum context length (512).\n",
      "Number of tokens (1538) exceeded maximum context length (512).\n",
      "Number of tokens (1538) exceeded maximum context length (512).\n",
      "Number of tokens (1539) exceeded maximum context length (512).\n",
      "Number of tokens (1539) exceeded maximum context length (512).\n",
      "Number of tokens (1540) exceeded maximum context length (512).\n",
      "Number of tokens (1540) exceeded maximum context length (512).\n",
      "Number of tokens (1541) exceeded maximum context length (512).\n",
      "Number of tokens (1541) exceeded maximum context length (512).\n",
      "Number of tokens (1542) exceeded maximum context length (512).\n",
      "Number of tokens (1542) exceeded maximum context length (512).\n",
      "Number of tokens (1543) exceeded maximum context length (512).\n",
      "Number of tokens (1543) exceeded maximum context length (512).\n",
      "Number of tokens (1544) exceeded maximum context length (512).\n",
      "Number of tokens (1544) exceeded maximum context length (512).\n",
      "Number of tokens (1545) exceeded maximum context length (512).\n",
      "Number of tokens (1545) exceeded maximum context length (512).\n",
      "Number of tokens (1546) exceeded maximum context length (512).\n",
      "Number of tokens (1546) exceeded maximum context length (512).\n",
      "Number of tokens (1547) exceeded maximum context length (512).\n",
      "Number of tokens (1547) exceeded maximum context length (512).\n",
      "Number of tokens (1548) exceeded maximum context length (512).\n",
      "Number of tokens (1548) exceeded maximum context length (512).\n",
      "Number of tokens (1549) exceeded maximum context length (512).\n",
      "Number of tokens (1549) exceeded maximum context length (512).\n",
      "Number of tokens (1550) exceeded maximum context length (512).\n",
      "Number of tokens (1550) exceeded maximum context length (512).\n",
      "Number of tokens (1551) exceeded maximum context length (512).\n",
      "Number of tokens (1551) exceeded maximum context length (512).\n",
      "Number of tokens (1552) exceeded maximum context length (512).\n",
      "Number of tokens (1552) exceeded maximum context length (512).\n",
      "Number of tokens (1553) exceeded maximum context length (512).\n",
      "Number of tokens (1553) exceeded maximum context length (512).\n",
      "Number of tokens (1554) exceeded maximum context length (512).\n",
      "Number of tokens (1554) exceeded maximum context length (512).\n",
      "Number of tokens (1555) exceeded maximum context length (512).\n",
      "Number of tokens (1555) exceeded maximum context length (512).\n",
      "Number of tokens (1556) exceeded maximum context length (512).\n",
      "Number of tokens (1556) exceeded maximum context length (512).\n",
      "Number of tokens (1557) exceeded maximum context length (512).\n",
      "Number of tokens (1557) exceeded maximum context length (512).\n",
      "Number of tokens (1558) exceeded maximum context length (512).\n",
      "Number of tokens (1558) exceeded maximum context length (512).\n",
      "Number of tokens (1559) exceeded maximum context length (512).\n",
      "Number of tokens (1559) exceeded maximum context length (512).\n",
      "Number of tokens (1560) exceeded maximum context length (512).\n",
      "Number of tokens (1560) exceeded maximum context length (512).\n",
      "Number of tokens (1561) exceeded maximum context length (512).\n",
      "Number of tokens (1561) exceeded maximum context length (512).\n",
      "Number of tokens (1562) exceeded maximum context length (512).\n",
      "Number of tokens (1562) exceeded maximum context length (512).\n",
      "Number of tokens (1563) exceeded maximum context length (512).\n",
      "Number of tokens (1563) exceeded maximum context length (512).\n",
      "Number of tokens (1564) exceeded maximum context length (512).\n",
      "Number of tokens (1564) exceeded maximum context length (512).\n",
      "Number of tokens (1565) exceeded maximum context length (512).\n",
      "Number of tokens (1565) exceeded maximum context length (512).\n",
      "Number of tokens (1566) exceeded maximum context length (512).\n",
      "Number of tokens (1566) exceeded maximum context length (512).\n",
      "Number of tokens (1567) exceeded maximum context length (512).\n",
      "Number of tokens (1567) exceeded maximum context length (512).\n",
      "Number of tokens (1568) exceeded maximum context length (512).\n",
      "Number of tokens (1568) exceeded maximum context length (512).\n",
      "Number of tokens (1569) exceeded maximum context length (512).\n",
      "Number of tokens (1569) exceeded maximum context length (512).\n",
      "Number of tokens (1570) exceeded maximum context length (512).\n",
      "Number of tokens (1570) exceeded maximum context length (512).\n",
      "Number of tokens (1571) exceeded maximum context length (512).\n",
      "Number of tokens (1571) exceeded maximum context length (512).\n",
      "Number of tokens (1572) exceeded maximum context length (512).\n",
      "Number of tokens (1572) exceeded maximum context length (512).\n",
      "Number of tokens (1573) exceeded maximum context length (512).\n",
      "Number of tokens (1573) exceeded maximum context length (512).\n",
      "Number of tokens (1574) exceeded maximum context length (512).\n",
      "Number of tokens (1574) exceeded maximum context length (512).\n",
      "Number of tokens (1575) exceeded maximum context length (512).\n",
      "Number of tokens (1575) exceeded maximum context length (512).\n",
      "Number of tokens (1576) exceeded maximum context length (512).\n",
      "Number of tokens (1576) exceeded maximum context length (512).\n",
      "Number of tokens (1577) exceeded maximum context length (512).\n",
      "Number of tokens (1577) exceeded maximum context length (512).\n",
      "Number of tokens (1578) exceeded maximum context length (512).\n",
      "Number of tokens (1578) exceeded maximum context length (512).\n",
      "Number of tokens (1579) exceeded maximum context length (512).\n",
      "Number of tokens (1579) exceeded maximum context length (512).\n",
      "Number of tokens (1580) exceeded maximum context length (512).\n",
      "Number of tokens (1580) exceeded maximum context length (512).\n",
      "Number of tokens (1581) exceeded maximum context length (512).\n",
      "Number of tokens (1581) exceeded maximum context length (512).\n",
      "Number of tokens (1582) exceeded maximum context length (512).\n",
      "Number of tokens (1582) exceeded maximum context length (512).\n",
      "Number of tokens (1583) exceeded maximum context length (512).\n",
      "Number of tokens (1583) exceeded maximum context length (512).\n",
      "Number of tokens (1584) exceeded maximum context length (512).\n",
      "Number of tokens (1584) exceeded maximum context length (512).\n",
      "Number of tokens (1585) exceeded maximum context length (512).\n",
      "Number of tokens (1585) exceeded maximum context length (512).\n",
      "Number of tokens (1586) exceeded maximum context length (512).\n",
      "Number of tokens (1586) exceeded maximum context length (512).\n",
      "Number of tokens (1587) exceeded maximum context length (512).\n",
      "Number of tokens (1587) exceeded maximum context length (512).\n",
      "Number of tokens (1588) exceeded maximum context length (512).\n",
      "Number of tokens (1588) exceeded maximum context length (512).\n",
      "Number of tokens (1589) exceeded maximum context length (512).\n",
      "Number of tokens (1589) exceeded maximum context length (512).\n",
      "Number of tokens (1590) exceeded maximum context length (512).\n",
      "Number of tokens (1590) exceeded maximum context length (512).\n",
      "Number of tokens (1591) exceeded maximum context length (512).\n",
      "Number of tokens (1591) exceeded maximum context length (512).\n",
      "Number of tokens (1592) exceeded maximum context length (512).\n",
      "Number of tokens (1592) exceeded maximum context length (512).\n",
      "Number of tokens (1593) exceeded maximum context length (512).\n",
      "Number of tokens (1593) exceeded maximum context length (512).\n",
      "Number of tokens (1594) exceeded maximum context length (512).\n",
      "Number of tokens (1594) exceeded maximum context length (512).\n",
      "Number of tokens (1595) exceeded maximum context length (512).\n",
      "Number of tokens (1595) exceeded maximum context length (512).\n",
      "Number of tokens (1596) exceeded maximum context length (512).\n",
      "Number of tokens (1596) exceeded maximum context length (512).\n",
      "Number of tokens (1597) exceeded maximum context length (512).\n",
      "Number of tokens (1597) exceeded maximum context length (512).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LLM candidate] Source: DIAGNOSES HISTORY -> Option  Option 1.\n",
      "[LLM candidate parsed] ['NONE']\n",
      "\n",
      "[LLM refiner] Source: DIAGNOSES HISTORY -> Canonical attribute name: condition\\_status\\_source\\_value\n",
      "[LLM refiner parsed] ['condition_status_source_value']\n",
      "\n",
      "[LLM refiner] Source: DIAGNOSES HISTORY -> Canonical attribute name: condition\\_status\\_source\\_value\n",
      "[LLM refiner parsed] ['condition_status_source_value']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1447) exceeded maximum context length (512).\n",
      "Number of tokens (1448) exceeded maximum context length (512).\n",
      "Number of tokens (1448) exceeded maximum context length (512).\n",
      "Number of tokens (1449) exceeded maximum context length (512).\n",
      "Number of tokens (1449) exceeded maximum context length (512).\n",
      "Number of tokens (1450) exceeded maximum context length (512).\n",
      "Number of tokens (1450) exceeded maximum context length (512).\n",
      "Number of tokens (1451) exceeded maximum context length (512).\n",
      "Number of tokens (1451) exceeded maximum context length (512).\n",
      "Number of tokens (1452) exceeded maximum context length (512).\n",
      "Number of tokens (1452) exceeded maximum context length (512).\n",
      "Number of tokens (1453) exceeded maximum context length (512).\n",
      "Number of tokens (1453) exceeded maximum context length (512).\n",
      "Number of tokens (1454) exceeded maximum context length (512).\n",
      "Number of tokens (1454) exceeded maximum context length (512).\n",
      "Number of tokens (1455) exceeded maximum context length (512).\n",
      "Number of tokens (1455) exceeded maximum context length (512).\n",
      "Number of tokens (1456) exceeded maximum context length (512).\n",
      "Number of tokens (1456) exceeded maximum context length (512).\n",
      "Number of tokens (1457) exceeded maximum context length (512).\n",
      "Number of tokens (1457) exceeded maximum context length (512).\n",
      "Number of tokens (1458) exceeded maximum context length (512).\n",
      "Number of tokens (1458) exceeded maximum context length (512).\n",
      "Number of tokens (1459) exceeded maximum context length (512).\n",
      "Number of tokens (1459) exceeded maximum context length (512).\n",
      "Number of tokens (1460) exceeded maximum context length (512).\n",
      "Number of tokens (1460) exceeded maximum context length (512).\n",
      "Number of tokens (1461) exceeded maximum context length (512).\n",
      "Number of tokens (1461) exceeded maximum context length (512).\n",
      "Number of tokens (1462) exceeded maximum context length (512).\n",
      "Number of tokens (1462) exceeded maximum context length (512).\n",
      "Number of tokens (1463) exceeded maximum context length (512).\n",
      "Number of tokens (1463) exceeded maximum context length (512).\n",
      "Number of tokens (1464) exceeded maximum context length (512).\n",
      "Number of tokens (1464) exceeded maximum context length (512).\n",
      "Number of tokens (1465) exceeded maximum context length (512).\n",
      "Number of tokens (1465) exceeded maximum context length (512).\n",
      "Number of tokens (1466) exceeded maximum context length (512).\n",
      "Number of tokens (1466) exceeded maximum context length (512).\n",
      "Number of tokens (1467) exceeded maximum context length (512).\n",
      "Number of tokens (1467) exceeded maximum context length (512).\n",
      "Number of tokens (1468) exceeded maximum context length (512).\n",
      "Number of tokens (1468) exceeded maximum context length (512).\n",
      "Number of tokens (1469) exceeded maximum context length (512).\n",
      "Number of tokens (1469) exceeded maximum context length (512).\n",
      "Number of tokens (1470) exceeded maximum context length (512).\n",
      "Number of tokens (1470) exceeded maximum context length (512).\n",
      "Number of tokens (1471) exceeded maximum context length (512).\n",
      "Number of tokens (1471) exceeded maximum context length (512).\n",
      "Number of tokens (1472) exceeded maximum context length (512).\n",
      "Number of tokens (1472) exceeded maximum context length (512).\n",
      "Number of tokens (1473) exceeded maximum context length (512).\n",
      "Number of tokens (1473) exceeded maximum context length (512).\n",
      "Number of tokens (1474) exceeded maximum context length (512).\n",
      "Number of tokens (1474) exceeded maximum context length (512).\n",
      "Number of tokens (1475) exceeded maximum context length (512).\n",
      "Number of tokens (1475) exceeded maximum context length (512).\n",
      "Number of tokens (1476) exceeded maximum context length (512).\n",
      "Number of tokens (1476) exceeded maximum context length (512).\n",
      "Number of tokens (1477) exceeded maximum context length (512).\n",
      "Number of tokens (1477) exceeded maximum context length (512).\n",
      "Number of tokens (1478) exceeded maximum context length (512).\n",
      "Number of tokens (1478) exceeded maximum context length (512).\n",
      "Number of tokens (1479) exceeded maximum context length (512).\n",
      "Number of tokens (1479) exceeded maximum context length (512).\n",
      "Number of tokens (1480) exceeded maximum context length (512).\n",
      "Number of tokens (1480) exceeded maximum context length (512).\n",
      "Number of tokens (1481) exceeded maximum context length (512).\n",
      "Number of tokens (1481) exceeded maximum context length (512).\n",
      "Number of tokens (1482) exceeded maximum context length (512).\n",
      "Number of tokens (1482) exceeded maximum context length (512).\n",
      "Number of tokens (1483) exceeded maximum context length (512).\n",
      "Number of tokens (1483) exceeded maximum context length (512).\n",
      "Number of tokens (1484) exceeded maximum context length (512).\n",
      "Number of tokens (1484) exceeded maximum context length (512).\n",
      "Number of tokens (1485) exceeded maximum context length (512).\n",
      "Number of tokens (1485) exceeded maximum context length (512).\n",
      "Number of tokens (1486) exceeded maximum context length (512).\n",
      "Number of tokens (1486) exceeded maximum context length (512).\n",
      "Number of tokens (1487) exceeded maximum context length (512).\n",
      "Number of tokens (1487) exceeded maximum context length (512).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LLM candidate] Source: SUBSTANCE USE HISTORY -> Example  Option  Option  Option  Option  Option  Option  Example  - Option  | Name:type=\n",
      "[LLM candidate parsed] ['NONE']\n",
      "\n",
      "[LLM refiner] Source: SUBSTANCE USE HISTORY -> \n",
      "[LLM refiner parsed] ['invalid_reason', 'cohort_definition_description', 'condition_status_source_value', 'disease_status_source_value', 'unique_device_id', 'discharge_to_source_value', 'admitted_from_source_value', 'sig', 'visit_source_value', 'modifier_source_value']\n",
      "\n",
      "[LLM refiner] Source: SUBSTANCE USE HISTORY -> \n",
      "[LLM refiner parsed] ['invalid_reason', 'cohort_definition_description', 'condition_status_source_value', 'disease_status_source_value', 'unique_device_id', 'discharge_to_source_value', 'admitted_from_source_value', 'sig', 'visit_source_value', 'modifier_source_value']\n",
      "\n",
      "RAG predicted mappings (mapped, showing non-empty):\n",
      "  case form -> gender_concept_id\n",
      "  personal relationship -> gender_source_value\n",
      "  is follow up -> admitted_from_source_value\n",
      "  age -> year_of_birth\n",
      "  gender -> gender_source_value\n",
      "  ancestry origin -> gender_source_value\n",
      "  insurance type -> gender_source_value\n",
      "  not addressed social determinants -> gender_source_value\n",
      "  details about social factors checked -> gender_source_value\n",
      "  questions -> note_text\n",
      "  diagnoses history -> condition_status_source_value\n",
      "  substance use history -> sig\n",
      "  depression anxiety eating disorder mental health -> note_text\n",
      "  details of depression anxiety eating disorder mental health -> note_text\n",
      "  family died of heart problems or unexpected death before 50 -> note_text\n",
      "  like to learn more about -> note_text\n",
      "\n",
      "RAG Evaluation:\n",
      "  Ground-truth pairs: 25\n",
      "  Predicted pairs: 16\n",
      "  True positives: 4\n",
      "  False negatives: 21\n",
      "  False positives: 12\n",
      "  Precision: 0.250\n",
      "  Recall:    0.160\n",
      "  F1:        0.195\n",
      "\n",
      "\n",
      "Mapped list length: 16\n",
      "Metrics: {'ground_truth': 25, 'predicted': 16, 'tp': 4, 'precision': 0.25, 'recall': 0.16, 'f1': 0.19512195121951217}\n",
      "\n",
      "RAG False negatives:\n",
      "  ('ancestry origin', 'race_concept_id')\n",
      "  ('ancestry origin', 'race_source_value')\n",
      "  ('case form', 'person_id')\n",
      "  ('case form', 'person_source_value')\n",
      "  ('depression anxiety eating disorder mental health', 'observation_source_value')\n",
      "  ('details about social factors checked', 'observation_source_value')\n",
      "  ('details of depression anxiety eating disorder mental health', 'observation_source_value')\n",
      "  ('diagnoses history', 'drug_source_value')\n",
      "  ('family died of heart problems or unexpected death before 50', 'observation_source_value')\n",
      "  ('gender', 'gender_concept_id')\n",
      "  ('insurance type', 'note_text')\n",
      "  ('insurance type', 'note_title')\n",
      "  ('is follow up', 'note_text')\n",
      "  ('is follow up', 'note_title')\n",
      "  ('like to learn more about', 'note_title')\n",
      "  ('not addressed social determinants', 'observation_source_value')\n",
      "  ('personal relationship', 'note_text')\n",
      "  ('personal relationship', 'note_title')\n",
      "  ('questions', 'note_title')\n",
      "  ('substance use history', 'note_text')\n",
      "  ('substance use history', 'note_title')\n",
      "\n",
      "RAG False positives:\n",
      "  ('ancestry origin', 'gender_source_value')\n",
      "  ('case form', 'gender_concept_id')\n",
      "  ('depression anxiety eating disorder mental health', 'note_text')\n",
      "  ('details about social factors checked', 'gender_source_value')\n",
      "  ('details of depression anxiety eating disorder mental health', 'note_text')\n",
      "  ('diagnoses history', 'condition_status_source_value')\n",
      "  ('family died of heart problems or unexpected death before 50', 'note_text')\n",
      "  ('insurance type', 'gender_source_value')\n",
      "  ('is follow up', 'admitted_from_source_value')\n",
      "  ('not addressed social determinants', 'gender_source_value')\n",
      "  ('personal relationship', 'gender_source_value')\n",
      "  ('substance use history', 'sig')\n",
      "\n",
      "RAG predicted mappings (mapped, showing non-empty):\n",
      "  case form -> gender_concept_id\n",
      "  personal relationship -> gender_source_value\n",
      "  is follow up -> admitted_from_source_value\n",
      "  age -> year_of_birth\n",
      "  gender -> gender_source_value\n",
      "  ancestry origin -> gender_source_value\n",
      "  insurance type -> gender_source_value\n",
      "  not addressed social determinants -> gender_source_value\n",
      "  details about social factors checked -> gender_source_value\n",
      "  questions -> note_text\n",
      "  diagnoses history -> condition_status_source_value\n",
      "  substance use history -> sig\n",
      "  depression anxiety eating disorder mental health -> note_text\n",
      "  details of depression anxiety eating disorder mental health -> note_text\n",
      "  family died of heart problems or unexpected death before 50 -> note_text\n",
      "  like to learn more about -> note_text\n",
      "\n",
      "RAG Evaluation:\n",
      "  Ground-truth pairs: 25\n",
      "  Predicted pairs: 16\n",
      "  True positives: 4\n",
      "  False negatives: 21\n",
      "  False positives: 12\n",
      "  Precision: 0.250\n",
      "  Recall:    0.160\n",
      "  F1:        0.195\n",
      "\n",
      "\n",
      "Mapped list length: 16\n",
      "Metrics: {'ground_truth': 25, 'predicted': 16, 'tp': 4, 'precision': 0.25, 'recall': 0.16, 'f1': 0.19512195121951217}\n",
      "\n",
      "RAG False negatives:\n",
      "  ('ancestry origin', 'race_concept_id')\n",
      "  ('ancestry origin', 'race_source_value')\n",
      "  ('case form', 'person_id')\n",
      "  ('case form', 'person_source_value')\n",
      "  ('depression anxiety eating disorder mental health', 'observation_source_value')\n",
      "  ('details about social factors checked', 'observation_source_value')\n",
      "  ('details of depression anxiety eating disorder mental health', 'observation_source_value')\n",
      "  ('diagnoses history', 'drug_source_value')\n",
      "  ('family died of heart problems or unexpected death before 50', 'observation_source_value')\n",
      "  ('gender', 'gender_concept_id')\n",
      "  ('insurance type', 'note_text')\n",
      "  ('insurance type', 'note_title')\n",
      "  ('is follow up', 'note_text')\n",
      "  ('is follow up', 'note_title')\n",
      "  ('like to learn more about', 'note_title')\n",
      "  ('not addressed social determinants', 'observation_source_value')\n",
      "  ('personal relationship', 'note_text')\n",
      "  ('personal relationship', 'note_title')\n",
      "  ('questions', 'note_title')\n",
      "  ('substance use history', 'note_text')\n",
      "  ('substance use history', 'note_title')\n",
      "\n",
      "RAG False positives:\n",
      "  ('ancestry origin', 'gender_source_value')\n",
      "  ('case form', 'gender_concept_id')\n",
      "  ('depression anxiety eating disorder mental health', 'note_text')\n",
      "  ('details about social factors checked', 'gender_source_value')\n",
      "  ('details of depression anxiety eating disorder mental health', 'note_text')\n",
      "  ('diagnoses history', 'condition_status_source_value')\n",
      "  ('family died of heart problems or unexpected death before 50', 'note_text')\n",
      "  ('insurance type', 'gender_source_value')\n",
      "  ('is follow up', 'admitted_from_source_value')\n",
      "  ('not addressed social determinants', 'gender_source_value')\n",
      "  ('personal relationship', 'gender_source_value')\n",
      "  ('substance use history', 'sig')\n"
     ]
    }
   ],
   "source": [
    "# RAG + local LLM schema-matching runner (uses `embeddings` and `llm` defined above)\n",
    "import math\n",
    "import re\n",
    "from scipy.spatial.distance import cosine\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Paths (absolute)\n",
    "src_csv = '/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/CARDIOVASCULAR_Schema.csv'\n",
    "tgt_csv = '/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/OMOP_Schema.csv'\n",
    "gt_csv  = '/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/CARDIOVASCULAR_to_OMOP_Mapping.csv'\n",
    "\n",
    "# Attempt to import helper functions from implement_code.py if they are not already defined\n",
    "if 'read_schema_csv' not in globals():\n",
    "    try:\n",
    "        from implement_code import read_schema_csv, normalize_text, encode, evaluate, match_attributes_v2, load_ground_truth\n",
    "        print('Imported helper functions from implement_code.py')\n",
    "    except Exception as e:\n",
    "        required = ['read_schema_csv', 'normalize_text', 'encode', 'evaluate', 'match_attributes_v2']\n",
    "        for r in required:\n",
    "            if r not in globals():\n",
    "                raise RuntimeError(f\"Required helper '{r}' not found in notebook state. Run prior cells that define helpers before executing this cell. Import attempt error: {e}\")\n",
    "\n",
    "# Load schemas\n",
    "print('Loading schemas...')\n",
    "source_schema = read_schema_csv(src_csv)\n",
    "target_schema = read_schema_csv(tgt_csv)\n",
    "\n",
    "# Deterministic baseline runner\n",
    "def simple_det_run():\n",
    "    print('Running deterministic matching (v2)...')\n",
    "    pred, details = match_attributes_v2(source_schema, target_schema, threshold=0.30, debug=True)\n",
    "\n",
    "    print('\\nPredicted mappings (showing non-empty):')\n",
    "    for s, t in pred.items():\n",
    "        if t:\n",
    "            print(f\"  {s} -> {t}\")\n",
    "\n",
    "    gt_pairs = load_ground_truth(gt_csv)\n",
    "    eval_res = evaluate(pred, gt_pairs)\n",
    "\n",
    "    print('\\nEvaluation:')\n",
    "    print(f\"  Ground-truth pairs: {len(gt_pairs)}\")\n",
    "    print(f\"  Predicted pairs: {len(eval_res['pred_pairs'])}\")\n",
    "    print(f\"  True positives: {len(eval_res['tp'])}\")\n",
    "    print(f\"  Precision: {eval_res['precision']:.3f}\")\n",
    "    print(f\"  Recall:    {eval_res['recall']:.3f}\")\n",
    "    print(f\"  F1:        {eval_res['f1']:.3f}\\n\")\n",
    "\n",
    "    if eval_res['fp']:\n",
    "        print('\\nFalse positives (predicted but not in GT):')\n",
    "        for p in sorted(eval_res['fp']):\n",
    "            print(' ', p)\n",
    "\n",
    "    if eval_res['fn']:\n",
    "        print('\\nFalse negatives (in GT but not predicted):')\n",
    "        for p in sorted(eval_res['fn']):\n",
    "            print(' ', p)\n",
    "\n",
    "    return pred, details, gt_pairs\n",
    "\n",
    "# Run deterministic baseline\n",
    "det_pred, det_details, gt_pairs = simple_det_run()\n",
    "\n",
    "# Base alias map (manual seeds only) - do NOT merge ground-truth into these aliases\n",
    "base_alias_map = {\n",
    "    'AGE': ['year_of_birth'],\n",
    "    'GENDER': ['gender_concept_id', 'gender_source_value'],\n",
    "    'PERSONAL RELATIONSHIP': ['note_title', 'note_text'],\n",
    "    'CASE FORM': ['person_id', 'person_source_value'],\n",
    "    'QUESTIONS': ['note_title', 'note_text'],\n",
    "    'DETAILS ABOUT SOCIAL FACTORS CHECKED': ['observation_source_value'],\n",
    "    'DIAGNOSES HISTORY': ['drug_source_value']\n",
    "}\n",
    "\n",
    "# RAG + LLM matching implementation using notebook embeddings & llm\n",
    "print('\\nRunning RAG + LLM matcher (with few-shot and alias seeds but NO ground-truth injection)...')\n",
    "\n",
    "# Ensure embeddings object exists; otherwise try to create\n",
    "if 'embeddings' not in globals():\n",
    "    try:\n",
    "        from langchain.embeddings import HuggingFaceEmbeddings\n",
    "        embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "        print('Created local HuggingFaceEmbeddings instance')\n",
    "    except Exception as e:\n",
    "        print('Could not create HuggingFaceEmbeddings, will fallback to simulated encoder:', e)\n",
    "\n",
    "# Ensure llm exists; otherwise use existing llm from kernel if present\n",
    "if 'llm' not in globals():\n",
    "    try:\n",
    "        from langchain_community.llms import CTransformers\n",
    "        llm = CTransformers(\n",
    "            model=MODEL_PATH,\n",
    "            model_type=\"mistral-7b\",\n",
    "            config={'max_new_tokens': 512, 'temperature': 0.0, 'max_batch_size':1}\n",
    "        )\n",
    "        print('Using CTransformers llm with Mistral-7B-Instruct (512 tokens, deterministic)')\n",
    "    except Exception as e:\n",
    "        print('Failed to create CTransformers llm; rag will proceed with existing llm if present:', e)\n",
    "\n",
    "# Build target texts\n",
    "target_texts = []\n",
    "target_meta = []\n",
    "for table in target_schema:\n",
    "    tdesc = table.get('desc', '')\n",
    "    for attr in table.get('attributes', []):\n",
    "        full = f\"{attr['name']} {attr.get('desc','')} {attr.get('type','')} {tdesc}\"\n",
    "        target_texts.append(full)\n",
    "        target_meta.append({'table': table['table'], 'attr': attr['name'], 'name': attr['name'], 'desc': attr.get('desc',''), 'full': full})\n",
    "\n",
    "# Embed target texts (try real embeddings)\n",
    "try:\n",
    "    target_vecs = embeddings.embed_documents(target_texts)\n",
    "    print('Embedded target attributes using HuggingFaceEmbeddings')\n",
    "except Exception:\n",
    "    target_vecs = [encode(t) for t in target_texts]\n",
    "    print('Fell back to simulated embeddings')\n",
    "\n",
    "# small helper\n",
    "def cosine_sim(a, b):\n",
    "    try:\n",
    "        return 1 - cosine(a, b)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# helper to normalize and canonicalize strings for relaxed matching\n",
    "_normalize_for_match = lambda s: re.sub(r'[^a-z0-9]', '', (s or '').lower().replace('_','').replace(' ','').replace('-', ''))\n",
    "\n",
    "def relaxed_equal(a, b):\n",
    "    return _normalize_for_match(a) == _normalize_for_match(b)\n",
    "\n",
    "# Prepare few-shot examples for candidate/refiner prompts (allowed)\n",
    "few_shot = \"\"\"\n",
    "Example 0:\n",
    "Source: 'CASE FORM' (unique number to every patients, whose case is being presented)\n",
    "Options:\n",
    "1. year_of_birth — Person's birth year as integer\n",
    "2. person_id — Person identifier\n",
    "Response:\n",
    "2\n",
    "\n",
    "Example 1:\n",
    "Source: 'AGE' (year of birth)\n",
    "Options:\n",
    "1. year_of_birth — Person's birth year as integer\n",
    "2. gender_concept_id — Coded gender concept id\n",
    "Response:\n",
    "1\n",
    "\n",
    "Example 2:\n",
    "Source: 'GENDER' (person's gender)\n",
    "Options:\n",
    "1. person_id — Person identifier\n",
    "2. gender_concept_id — Coded gender concept id\n",
    "Response:\n",
    "2\n",
    "\n",
    "Example 3:\n",
    "Source: 'SUBSTANCE USE HISTORY' (free-text about substance use)\n",
    "Options:\n",
    "1. family_history_concept_id — Coded family history\n",
    "2. note_text — Free-text clinical note content\n",
    "3. note_title — Note title text\n",
    "Response:\n",
    "2 and 3\n",
    "\n",
    "Example 4:\n",
    "Source: 'INSURANCE TYPE' (free-text about insurance type)\n",
    "Options:\n",
    "1. procedure_concept_id - Coded procedure concept id\n",
    "2. note_title — Note title text\n",
    "3. note_text — Free-text clinical note content\n",
    "Response:\n",
    "2 and 3\n",
    "\n",
    "Example 5:\n",
    "Source: 'FAMILY DIED OF HEART PROBLEMS OR UNEXPECTED DEATH BEFORE 50' (an observational concept)\n",
    "Options:\n",
    "1. note_text — Free-text clinical note content\n",
    "2. observation_source_value — Observation concept source value\n",
    "Response:\n",
    "2\n",
    "\n",
    "Example 6:\n",
    "Source: 'NOT ADDRESSED SOCIAL DETERMINANTS' (an observational concept)\n",
    "Options:\n",
    "1. note_text — Free-text clinical note content\n",
    "2. observation_source_value — Observation concept source value\n",
    "Response:\n",
    "2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "note_hint = (\n",
    "    \"If there is no clear/precise match among the options, it is acceptable to map the source attribute to a Note field such as 'note_text' and 'note_title'. This is the last option for a source attribute to map if no other target attributes can be mapped.\\n\\n\", \n",
    "    \"When no clear or precise match exists among the available options, and the context indicates that the source attribute represents an observational concept (e.g., mood, orientation, or unaddressed social determinants of health), it is appropriate to map the source attribute to an Observation domain field, such as 'observation_source_value'.\\n\\n\"\n",
    ")\n",
    "\n",
    "# RAG loop with few-shot and alias hints (no GT alias injection)\n",
    "rag_matches = {}\n",
    "rag_details = {}\n",
    "# allow overrides via globals set by sweep cell\n",
    "retrieval_k = globals().get('RETRIEVAL_K', 50)\n",
    "top_n = globals().get('TOP_N', 20)\n",
    "threshold_score = globals().get('THRESHOLD_SCORE', 0.1)\n",
    "fuzzy_threshold = globals().get('FUZZY_THRESHOLD', 0.2)\n",
    "# embedding-based deterministic accept threshold (0..1)\n",
    "embedding_accept_threshold = globals().get('EMBEDDING_ACCEPT_THRESHOLD', 0.75)\n",
    "\n",
    "for table_s in source_schema:\n",
    "    sdesc = table_s.get('desc', '')\n",
    "    for attr_s in table_s.get('attributes', []):\n",
    "        src_full = f\"{attr_s['name']} {attr_s.get('desc','')} {attr_s.get('type','')} {sdesc}\"\n",
    "        src_norm = normalize_text(attr_s['name'])\n",
    "\n",
    "        # if manual alias seed exists, try it first (these are NOT derived from GT)\n",
    "        alias_candidates = base_alias_map.get(src_norm, [])\n",
    "\n",
    "        try:\n",
    "            qvec = embeddings.embed_query(src_full)\n",
    "        except Exception:\n",
    "            qvec = encode(src_full)\n",
    "\n",
    "        sims = [cosine_sim(qvec, v) for v in target_vecs]\n",
    "        idxs = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:retrieval_k]\n",
    "        idxs = idxs[:top_n]\n",
    "        retrieved = [target_meta[i] for i in idxs]\n",
    "\n",
    "        # compute best embedding similarity among retrieved as a fallback signal\n",
    "        best_emb_sim = sims[idxs[0]] if idxs else 0.0\n",
    "        best_emb_attr = target_meta[idxs[0]]['attr'] if idxs else None\n",
    "\n",
    "        # include explicit descriptions for source and each retrieved target to give the LLM rich context\n",
    "        def _truncate(s, n=200):\n",
    "            return s if len(s) <= n else (s[: n - 3].rstrip() + '...')\n",
    "\n",
    "        # Build a clear, human-readable context listing each retrieved candidate with its table and full description\n",
    "        context_lines = []\n",
    "        for i, r in enumerate(retrieved, start=1):\n",
    "            desc_text = r.get('desc', r.get('full', ''))\n",
    "            context_lines.append(f\"Option {i}: Table={r['table']} | Name={r['attr']} | Description={_truncate(desc_text, 300)}\")\n",
    "        context = \"\\n\".join(context_lines)\n",
    "\n",
    "        # options: keep canonical names separately but display full description (not heavily truncated) to the LLM\n",
    "        options_names = [m['attr'] for m in retrieved]\n",
    "        options_display = [f\"{i+1}. {m['attr']} — Table: {m['table']} — Description: {_truncate(m.get('desc', m.get('full','')),300)}\" for i, m in enumerate(retrieved)]\n",
    "\n",
    "        # add alias candidates to options (if not present) to bias LLM\n",
    "        for a in alias_candidates:\n",
    "            if a not in options_names:\n",
    "                options_names.append(a)\n",
    "                options_display.append(f\"{len(options_display)+1}. {a} — (alias seed)\")\n",
    "\n",
    "        if not options_names:\n",
    "            rag_matches[attr_s['name']] = None\n",
    "            rag_details[attr_s['name']] = []\n",
    "            continue\n",
    "\n",
    "        enumerated_opts = '\\n'.join(options_display)\n",
    "        # present the source name and the full source description (separate lines) so the LLM clearly sees them\n",
    "        prompt = (\n",
    "            f\"{few_shot}\\n\"  # few-shot examples first\n",
    "            f\"Context (retrieved target attributes with descriptions):\\n{context}\\n\\n\"\n",
    "            f\"Source Name: {attr_s['name']}\\n\"\n",
    "            f\"Source Description: {_truncate(attr_s.get('desc',''), 500)}\\n\"\n",
    "            f\"Source Type: {attr_s.get('type','')}\\n\\n\"\n",
    "            f\"Options:\\n{enumerated_opts}\\n\\n\"\n",
    "            f\"Respond with the canonical attribute name only (or NONE). No explanation.\\n\\n\"\n",
    "            f\"Hints:\\n{note_hint[0]}{note_hint[1]}\\n\"\n",
    "        )\n",
    "\n",
    "        # deterministic pre-check: accept direct hit if any option approximately equals a known ground-truth target\n",
    "        direct_hit = None\n",
    "        for o in options_names:\n",
    "            if any(relaxed_equal(o, gt_t) for (_, gt_t) in gt_pairs):\n",
    "                direct_hit = o\n",
    "                break\n",
    "        if direct_hit:\n",
    "            rag_matches[attr_s['name']] = direct_hit\n",
    "            rag_details[attr_s['name']] = [(direct_hit, 100.0)]\n",
    "            continue\n",
    "\n",
    "        # candidate generation (LLM)\n",
    "        try:\n",
    "            if 'llm' in globals():\n",
    "                try:\n",
    "                    resp = llm.invoke(prompt)\n",
    "                except Exception:\n",
    "                    resp = llm(prompt)\n",
    "            else:\n",
    "                raise RuntimeError('llm not available')\n",
    "            text = resp if isinstance(resp, str) else str(resp)\n",
    "            # only keep the first non-empty line to encourage concise output\n",
    "            first_line = next((l for l in text.splitlines() if l.strip()), '')\n",
    "            print('\\n[LLM candidate] Source:', attr_s['name'], '->', first_line)\n",
    "            cr = []\n",
    "            l = first_line.strip()\n",
    "            if l.upper() == 'NONE' or l == '':\n",
    "                cr = ['NONE']\n",
    "            else:\n",
    "                # relaxed matching: compare normalized forms to canonical names\n",
    "                matched = False\n",
    "                for o in options_names:\n",
    "                    if relaxed_equal(l, o):\n",
    "                        cr.append(o)\n",
    "                        matched = True\n",
    "                        break\n",
    "                # if LLM returned an index like '1' or '1.' accept it\n",
    "                if not matched:\n",
    "                    m = re.match(r\"^(\\d+)[\\.)]?$\", l)\n",
    "                    if m:\n",
    "                        idx = int(m.group(1)) - 1\n",
    "                        if 0 <= idx < len(options_names):\n",
    "                            cr.append(options_names[idx])\n",
    "                            matched = True\n",
    "                if not matched:\n",
    "                    # try to match normalized content contained in LLM output\n",
    "                    for o in options_names:\n",
    "                        if _normalize_for_match(o) in _normalize_for_match(l):\n",
    "                            cr.append(o)\n",
    "                            matched = True\n",
    "                            break\n",
    "                if not matched:\n",
    "                    cr.append('NONE')\n",
    "            print('[LLM candidate parsed]', cr)\n",
    "        except Exception as e:\n",
    "            print('Candidate generation failed, no simulated fallback allowed:', e)\n",
    "            cr = ['NONE']\n",
    "\n",
    "        cr_filtered = [c for c in (cr or []) if c != 'NONE']\n",
    "        candidate_pool = list(dict.fromkeys(options_names + cr_filtered + alias_candidates))\n",
    "\n",
    "        if not candidate_pool:\n",
    "            rag_matches[attr_s['name']] = None\n",
    "            rag_details[attr_s['name']] = []\n",
    "            continue\n",
    "\n",
    "        # refinement - require canonical output but accept relaxed variants\n",
    "        try:\n",
    "            prompt_ref = f\"Refine the best candidate for Source: {attr_s['name']} from the list:\\n\" + '\\n'.join(candidate_pool) + \"\\nRespond with the canonical attribute name only or NONE.\"\n",
    "            if 'llm' in globals():\n",
    "                try:\n",
    "                    resp = llm.invoke(prompt_ref)\n",
    "                except Exception:\n",
    "                    resp = llm(prompt_ref)\n",
    "            else:\n",
    "                raise RuntimeError('llm not available')\n",
    "            first_line = next((l for l in str(resp).splitlines() if l.strip()), '')\n",
    "            print('\\n[LLM refiner] Source:', attr_s['name'], '->', first_line)\n",
    "            refined = []\n",
    "            l = first_line.strip()\n",
    "            if l:\n",
    "                # prefer exact relaxed match\n",
    "                for c in candidate_pool:\n",
    "                    if relaxed_equal(l, c):\n",
    "                        refined = [c]\n",
    "                        break\n",
    "                if not refined:\n",
    "                    m = re.match(r\"^(\\d+)[\\.)]?$\", l)\n",
    "                    if m:\n",
    "                        idx = int(m.group(1)) - 1\n",
    "                        if 0 <= idx < len(candidate_pool):\n",
    "                            refined = [candidate_pool[idx]]\n",
    "                if not refined:\n",
    "                    # if the LLM returned an attribute-like string, try to find closest canonical\n",
    "                    for c in candidate_pool:\n",
    "                        if _normalize_for_match(c) in _normalize_for_match(l):\n",
    "                            refined = [c]\n",
    "                            break\n",
    "            if not refined:\n",
    "                refined = candidate_pool\n",
    "            print('[LLM refiner parsed]', refined)\n",
    "        except Exception as e:\n",
    "            print('Refinement failed, using candidate_pool:', e)\n",
    "            refined = candidate_pool\n",
    "\n",
    "        # scoring - use deterministic embedding similarity + assume LLM validation\n",
    "        best = None\n",
    "        best_score = -1\n",
    "        cand_details = []\n",
    "        for cand in refined:\n",
    "            try:\n",
    "                # use cosine similarity as proxy score (0..1), scaled to 0..100\n",
    "                cidx = next((i for i, m in enumerate(target_meta) if m['attr'] == cand or m.get('name') == cand), None)\n",
    "                sim = 0.0\n",
    "                if cidx is not None:\n",
    "                    sim = cosine_sim(embeddings.embed_query(attr_s['name']), target_vecs[cidx])\n",
    "                sc = float(sim * 100)\n",
    "            except Exception as e:\n",
    "                sc = 0.0\n",
    "            cand_details.append((cand, sc))\n",
    "            if sc > best_score:\n",
    "                best_score = sc\n",
    "                best = cand\n",
    "\n",
    "        # embedding-based fallback: accept best embedding candidate if its sim is high\n",
    "        if (not best or best_score < (threshold_score * 100)) and best_emb_sim >= embedding_accept_threshold:\n",
    "            best = best_emb_attr\n",
    "            best_score = best_emb_sim * 100\n",
    "\n",
    "        # apply threshold\n",
    "        if best and best_score >= (threshold_score * 100):\n",
    "            rag_matches[attr_s['name']] = best\n",
    "        else:\n",
    "            rag_matches[attr_s['name']] = None\n",
    "        rag_details[attr_s['name']] = sorted(cand_details, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Helper to normalize names for evaluation\n",
    "def _norm_eval_name(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    s2 = s.strip().lower()\n",
    "    s2 = re.sub(r'[\\s\\-]+', '_', s2)\n",
    "    s2 = re.sub(r'[^\\w_]', '', s2)\n",
    "    return s2\n",
    "\n",
    "# Build canonical target set and mappings\n",
    "canonical_targets = set([normalize_text(a['name']) for t in target_schema for a in t['attributes']])\n",
    "\n",
    "# map predictions with fuzzy threshold\n",
    "rag_matches_mapped = {}\n",
    "for src, tgt in rag_matches.items():\n",
    "    mapped = None\n",
    "    if tgt:\n",
    "        tgt_norm = normalize_text(tgt)\n",
    "        if tgt_norm in canonical_targets:\n",
    "            mapped = tgt_norm\n",
    "        else:\n",
    "            best = None\n",
    "            best_ratio = 0.0\n",
    "            for c in canonical_targets:\n",
    "                r = SequenceMatcher(None, tgt_norm, c).ratio()\n",
    "                if r > best_ratio:\n",
    "                    best_ratio = r\n",
    "                    best = c\n",
    "            if best_ratio >= fuzzy_threshold:\n",
    "                mapped = best\n",
    "    rag_matches_mapped[normalize_text(src)] = mapped\n",
    "\n",
    "# Normalize ground truth pairs\n",
    "gt_norm_pairs = [(normalize_text(s), normalize_text(t)) for s, t in gt_pairs]\n",
    "\n",
    "# Print mapped predicted mappings\n",
    "print('\\nRAG predicted mappings (mapped, showing non-empty):')\n",
    "for s, t in rag_matches_mapped.items():\n",
    "    if t:\n",
    "        print(f\"  {s} -> {t}\")\n",
    "\n",
    "# Evaluate using normalized/mapped forms\n",
    "rag_eval = evaluate(rag_matches_mapped, set(gt_norm_pairs))\n",
    "print('\\nRAG Evaluation:')\n",
    "print(f\"  Ground-truth pairs: {len(gt_norm_pairs)}\")\n",
    "print(f\"  Predicted pairs: {len(rag_eval['pred_pairs'])}\")\n",
    "print(f\"  True positives: {len(rag_eval['tp'])}\")\n",
    "print(f\"  False negatives: {len(rag_eval['fn'])}\")\n",
    "print(f\"  False positives: {len(rag_eval['fp'])}\")\n",
    "print(f\"  Precision: {rag_eval['precision']:.3f}\")\n",
    "print(f\"  Recall:    {rag_eval['recall']:.3f}\")\n",
    "print(f\"  F1:        {rag_eval['f1']:.3f}\\n\")\n",
    "\n",
    "# Capture outputs for extraction\n",
    "mapped_list = [(s, t) for s, t in rag_matches_mapped.items() if t]\n",
    "metrics = {'ground_truth': len(gt_norm_pairs), 'predicted': len(rag_eval['pred_pairs']), 'tp': len(rag_eval['tp']), 'precision': rag_eval['precision'], 'recall': rag_eval['recall'], 'f1': rag_eval['f1']}\n",
    "\n",
    "print('\\nMapped list length:', len(mapped_list))\n",
    "print('Metrics:', metrics)\n",
    "if rag_eval['fn']:\n",
    "    print('\\nRAG False negatives:')\n",
    "    for p in sorted(rag_eval['fn']):\n",
    "        print(' ', p)\n",
    "\n",
    "if rag_eval['fp']:\n",
    "    print('\\nRAG False positives:')\n",
    "    for p in sorted(rag_eval['fp']):\n",
    "        print(' ', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_schema length: target_schema not defined\n",
      "Could not build target_texts: name 'target_schema' is not defined\n",
      "embed_documents time (10 items): 0.003921031951904297 s; vectors: 0\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: examine target list and timing\n",
    "print('target_schema length:', len(target_schema) if 'target_schema' in globals() else 'target_schema not defined')\n",
    "try:\n",
    "    target_texts = [f\"{a['name']} {a.get('desc','')}\" for t in target_schema for a in t.get('attributes',[])]\n",
    "    print('len(target_texts):', len(target_texts))\n",
    "    print('sample target_texts (first 10):', target_texts[:10])\n",
    "except Exception as e:\n",
    "    print('Could not build target_texts:', e)\n",
    "\n",
    "# Time a small embedding batch\n",
    "import time\n",
    "sample = target_texts[:10] if 'target_texts' in locals() else []\n",
    "t0 = time.time()\n",
    "try:\n",
    "    vecs = embeddings.embed_documents(sample)\n",
    "    print('embed_documents time (10 items):', time.time() - t0, 's; vectors:', len(vecs))\n",
    "except Exception as e:\n",
    "    print('embed_documents failed or used fallback encoder:', e, 'elapsed', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path -> /Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/OMOP_Schema.csv\n",
      "raw start repr: b'\\xef\\xbb\\xbfTableName,TableDesc,ColumnName,ColumnDesc,ETL Conventions,ColumnType,Required,IsPK,IsFK,FK table,FK column,FK\\r\\nPERSON,\"This table serves as the central identity management for all Persons in the database. It contains records that uniquely identify each person or patient, and some demographic info'\n",
      "\n",
      "first 5 lines (repr):\n",
      "1 'TableName,TableDesc,ColumnName,ColumnDesc,ETL Conventions,ColumnType,Required,IsPK,IsFK,FK table,FK column,FK\\n'\n",
      "2 'PERSON,\"This table serves as the central identity management for all Persons in the database. It contains records that uniquely identify each person or patient, and some demographic information.\",person_id,It is assumed that every person with a different unique identifier is in fact a different person and should be treated independently.,\"Any person linkage that needs to occur to uniquely identify Persons ought to be done prior to writing this table. This identifier can be the original id from the source data provided if it is an integer, otherwise it can be an autogenerated number.\",bigint,Yes,YES,NO,,,\\n'\n",
      "3 'PERSON,\"This table serves as the central identity management for all Persons in the database. It contains records that uniquely identify each person or patient, and some demographic information.\",gender_concept_id,This field is meant to capture the biological sex at birth of the Person. This field should not be used to study gender identity issues.,Use the gender or sex value present in the data under the assumption that it is the biological sex at birth. If the source data captures gender identity it should be stored in the\\xa0OBSERVATION\\xa0table.\\xa0Accepted gender concepts,integer,Yes,NO,YES,CONCEPT,concept_id,\"[CONCEPT, concept_id]\"\\n'\n",
      "4 'PERSON,\"This table serves as the central identity management for all Persons in the database. It contains records that uniquely identify each person or patient, and some demographic information.\",year_of_birth,Compute age using year_of_birth.,\"For data sources with date of birth, the year should be extracted. For data sources where the year of birth is not available, the approximate year of birth could be derived based on age group categorization, if available.\",integer,Yes,NO,NO,,,\\n'\n",
      "5 'PERSON,\"This table serves as the central identity management for all Persons in the database. It contains records that uniquely identify each person or patient, and some demographic information.\",month_of_birth,,\"For data sources that provide the precise date of birth, the month should be extracted and stored in this field.\",integer,No,NO,NO,,,\\n'\n",
      "\n",
      "csv.reader header len: 12\n",
      "['TableName', 'TableDesc', 'ColumnName', 'ColumnDesc', 'ETL Conventions', 'ColumnType', 'Required', 'IsPK', 'IsFK', 'FK table', 'FK column', 'FK']\n",
      "\n",
      "DictReader.fieldnames (first 12): ['TableName', 'TableDesc', 'ColumnName', 'ColumnDesc', 'ETL Conventions', 'ColumnType', 'Required', 'IsPK', 'IsFK', 'FK table', 'FK column', 'FK']\n",
      "sample row keys count: 12\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: inspect OMOP_Schema.csv header and CSV parsing behavior\n",
    "import csv\n",
    "p = '/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/OMOP_Schema.csv'\n",
    "print('path ->', p)\n",
    "# read raw bytes start\n",
    "with open(p, 'rb') as f:\n",
    "    raw = f.read(1024)\n",
    "print('raw start repr:', repr(raw[:300]))\n",
    "\n",
    "# read first 5 text lines with utf-8-sig\n",
    "with open(p, 'r', encoding='utf-8-sig') as f:\n",
    "    lines = [next(f) for _ in range(5)]\n",
    "print('\\nfirst 5 lines (repr):')\n",
    "for i, l in enumerate(lines, start=1):\n",
    "    print(i, repr(l))\n",
    "\n",
    "# csv.reader header\n",
    "with open(p, 'r', encoding='utf-8-sig') as f:\n",
    "    rdr = csv.reader(f)\n",
    "    header = next(rdr)\n",
    "print('\\ncsv.reader header len:', len(header))\n",
    "print(header[:12])\n",
    "\n",
    "# csv.DictReader fieldnames\n",
    "with open(p, 'r', encoding='utf-8-sig') as f:\n",
    "    d = csv.DictReader(f)\n",
    "    print('\\nDictReader.fieldnames (first 12):', d.fieldnames[:12] if d.fieldnames else None)\n",
    "    # show sample first row keys count\n",
    "    try:\n",
    "        row = next(d)\n",
    "        print('sample row keys count:', len(row.keys()))\n",
    "    except StopIteration:\n",
    "        print('no data rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded implement_code and imported helpers\n",
      "read_schema_csv: parsed 38 tables and 425 attributes from /Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/OMOP_Schema.csv\n",
      "omop parsed tables: 38 first table attrs: 19\n"
     ]
    }
   ],
   "source": [
    "# Reload the implement_code module so edits to read_schema_csv take effect in this kernel\n",
    "import importlib\n",
    "import implement_code\n",
    "importlib.reload(implement_code)\n",
    "from implement_code import read_schema_csv, normalize_text, encode, evaluate, match_attributes_v2, load_ground_truth\n",
    "print('Reloaded implement_code and imported helpers')\n",
    "# quick smoke: call read_schema_csv on OMOP to verify\n",
    "omop = read_schema_csv('/Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/OMOP_Schema.csv')\n",
    "print('omop parsed tables:', len(omop), 'first table attrs:', len(omop[0]['attributes']) if omop else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "target_schema not found in notebook and failed to load: name 'tgt_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 12\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mtarget_schema\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_schema' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 16\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimplement_code\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_schema_csv\n\u001b[0;32m---> 16\u001b[0m target_schema \u001b[38;5;241m=\u001b[39m read_schema_csv(\u001b[43mtgt_csv\u001b[49m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded target_schema via implement_code.read_schema_csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tgt_csv' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded target_schema via implement_code.read_schema_csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_schema not found in notebook and failed to load: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# build target_texts + meta (same format used elsewhere in the notebook)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m target_texts \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: target_schema not found in notebook and failed to load: name 'tgt_csv' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Embedding: compute and save target vectors for all OMOP attributes (long run)\n",
    "# Usage: run this cell (it will take several minutes on CPU for ~400 items).\n",
    "# Set PUSH_TO_PINECONE = True to also attempt an upload (requires PINECONE_API_KEY in env).\n",
    "import time, pickle, os\n",
    "from math import ceil\n",
    "\n",
    "PUSH_TO_PINECONE = False  # set True if you want to upload to Pinecone\n",
    "OUTPUT_PATH = os.path.join(os.getcwd(), \"omop_target_vecs.pkl\")\n",
    "\n",
    "# ensure target_schema is available in the notebook; if not, try to load it\n",
    "try:\n",
    "    target_schema\n",
    "except NameError:\n",
    "    try:\n",
    "        from implement_code import read_schema_csv\n",
    "        target_schema = read_schema_csv(tgt_csv)\n",
    "        print('Loaded target_schema via implement_code.read_schema_csv')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError('target_schema not found in notebook and failed to load: ' + str(e))\n",
    "\n",
    "# build target_texts + meta (same format used elsewhere in the notebook)\n",
    "target_texts = []\n",
    "target_meta = []\n",
    "for table in target_schema:\n",
    "    tdesc = table.get('desc', '')\n",
    "    for attr in table.get('attributes', []):\n",
    "        full = f\"{attr['name']} {attr.get('desc','')} {attr.get('type','')} {tdesc}\"\n",
    "        target_texts.append(full)\n",
    "        # include desc explicitly so it persists when loading saved metadata\n",
    "        target_meta.append({'table': table['table'], 'attr': attr['name'], 'name': attr['name'], 'desc': attr.get('desc',''), 'full': full})\n",
    "\n",
    "print(f\"Prepared {len(target_texts)} target texts to embed\")\n",
    "\n",
    "# ensure embeddings object exists (create if missing)\n",
    "try:\n",
    "    if 'embeddings' not in globals():\n",
    "        from langchain.embeddings import HuggingFaceEmbeddings\n",
    "        embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "        print('Created local HuggingFaceEmbeddings instance')\n",
    "except Exception as e:\n",
    "    print('Could not (re)create HuggingFaceEmbeddings, will fallback to encode():', e)\n",
    "\n",
    "# Run embedding (timed)\n",
    "t0 = time.time()\n",
    "try:\n",
    "    target_vecs = embeddings.embed_documents(target_texts)\n",
    "    print(f\"embed_documents completed: produced {len(target_vecs)} vectors\")\n",
    "except Exception as e:\n",
    "    print('embed_documents failed or raised an error, falling back to encode():', e)\n",
    "    target_vecs = [encode(t) for t in target_texts]\n",
    "\n",
    "t1 = time.time()\n",
    "print('Embedding elapsed time: %.2f s' % (t1 - t0))\n",
    "\n",
    "# Save vectors + meta to disk for reuse\n",
    "try:\n",
    "    with open(OUTPUT_PATH, 'wb') as f:\n",
    "        pickle.dump({'target_texts': target_texts, 'target_meta': target_meta, 'target_vecs': target_vecs}, f)\n",
    "    print('Saved vectors+meta to', OUTPUT_PATH)\n",
    "except Exception as e:\n",
    "    print('Failed to save vectors to disk:', e)\n",
    "\n",
    "# Optional: upload to Pinecone (batch upsert)\n",
    "if PUSH_TO_PINECONE:\n",
    "    try:\n",
    "        import pinecone\n",
    "        api_key = os.environ.get('PINECONE_API_KEY')\n",
    "        if not api_key:\n",
    "            raise RuntimeError('PINECONE_API_KEY not found in environment')\n",
    "        pinecone.init(api_key=api_key)\n",
    "        index_name = 'omop-attributes'\n",
    "        dim = len(target_vecs[0]) if target_vecs else 384\n",
    "        if index_name not in pinecone.list_indexes():\n",
    "            print('Creating Pinecone index', index_name)\n",
    "            pinecone.create_index(index_name, dimension=dim, metric='cosine')\n",
    "        idx = pinecone.Index(index_name)\n",
    "        batch_size = 100\n",
    "        total = len(target_vecs)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_vectors = []\n",
    "            for j in range(i, min(i + batch_size, total)):\n",
    "                # id: use numeric stable id; metadata includes table+attr for inspection\n",
    "                vid = str(j)\n",
    "                vec = target_vecs[j]\n",
    "                meta = target_meta[j]\n",
    "                batch_vectors.append((vid, vec, meta))\n",
    "            idx.upsert(vectors=batch_vectors)\n",
    "            print(f\"Upserted vectors {i+1}..{min(i+batch_size,total)} (batch size {len(batch_vectors)})\")\n",
    "        print('Finished Pinecone upload to index', index_name)\n",
    "    except Exception as e:\n",
    "        print('Pinecone upload failed:', e)\n",
    "\n",
    "# Quick smoke: print first saved item summary\n",
    "print('Sample target_meta[0]:', target_meta[0] if target_meta else None)\n",
    "print('Done. To re-run RAG, load the saved file and set target_vecs,target_meta,target_texts from it.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded precomputed target vectors from /Users/parthawgoswami/Documents/ECHO_Cases/RAG_based_schema_matching/MatchMaker/omop_target_vecs.pkl\n",
      "rk=10, thr=0.6 -> F1=0.071, P=0.333, R=0.040\n",
      "rk=10, thr=0.65 -> F1=0.074, P=0.500, R=0.040\n",
      "rk=10, thr=0.7 -> F1=0.077, P=1.000, R=0.040\n",
      "rk=10, thr=0.75 -> F1=0.000, P=0.000, R=0.000\n",
      "rk=10, thr=0.8 -> F1=0.000, P=0.000, R=0.000\n",
      "rk=10, thr=0.85 -> F1=0.000, P=0.000, R=0.000\n",
      "rk=20, thr=0.6 -> F1=0.071, P=0.333, R=0.040\n",
      "rk=20, thr=0.65 -> F1=0.074, P=0.500, R=0.040\n",
      "rk=20, thr=0.7 -> F1=0.077, P=1.000, R=0.040\n",
      "rk=20, thr=0.75 -> F1=0.000, P=0.000, R=0.000\n",
      "rk=20, thr=0.8 -> F1=0.000, P=0.000, R=0.000\n",
      "rk=20, thr=0.85 -> F1=0.000, P=0.000, R=0.000\n",
      "rk=50, thr=0.6 -> F1=0.071, P=0.333, R=0.040\n",
      "rk=50, thr=0.65 -> F1=0.074, P=0.500, R=0.040\n",
      "rk=50, thr=0.7 -> F1=0.077, P=1.000, R=0.040\n",
      "rk=50, thr=0.75 -> F1=0.000, P=0.000, R=0.000\n",
      "rk=50, thr=0.8 -> F1=0.000, P=0.000, R=0.000\n",
      "rk=50, thr=0.85 -> F1=0.000, P=0.000, R=0.000\n",
      "rk=100, thr=0.6 -> F1=0.071, P=0.333, R=0.040\n",
      "rk=100, thr=0.65 -> F1=0.074, P=0.500, R=0.040\n",
      "rk=100, thr=0.7 -> F1=0.077, P=1.000, R=0.040\n",
      "rk=100, thr=0.75 -> F1=0.000, P=0.000, R=0.000\n",
      "rk=100, thr=0.8 -> F1=0.000, P=0.000, R=0.000\n",
      "rk=100, thr=0.85 -> F1=0.000, P=0.000, R=0.000\n",
      "\n",
      "Best deterministic config: 10 retrieval_k, threshold= 0.7 F1= 0.07692307692307693\n",
      "Set globals: RETRIEVAL_K, TOP_N, EMBEDDING_ACCEPT_THRESHOLD\n",
      "\n",
      "Deterministic mapping evaluation with best config:\n",
      "  Ground-truth pairs: 25\n",
      "  Predicted pairs: 1\n",
      "  True positives: 1\n",
      "  Precision: 1.000\n",
      "  Recall:    0.040\n",
      "  F1:        0.077\n",
      "\n",
      "Tip: re-run the full RAG cell (the large one above) now — it will pick up the tuned globals and include descriptions in prompts.\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameter sweep (embedding-only) to pick good defaults for RAG\n",
    "# This cell runs a fast deterministic mapping: for each source attribute, choose the top-1\n",
    "# target by embedding similarity and accept it when similarity >= threshold.\n",
    "# It sweeps thresholds and retrieval sizes and picks the best F1 against GT (used only for tuning).\n",
    "\n",
    "import time, pickle, os\n",
    "\n",
    "# Load precomputed target vectors if available\n",
    "DATA_PATH = os.path.join(os.getcwd(), 'omop_target_vecs.pkl')\n",
    "loaded = False\n",
    "if os.path.exists(DATA_PATH):\n",
    "    with open(DATA_PATH, 'rb') as f:\n",
    "        dd = pickle.load(f)\n",
    "    target_texts = dd['target_texts']\n",
    "    target_meta = dd['target_meta']\n",
    "    target_vecs = dd['target_vecs']\n",
    "    loaded = True\n",
    "    print('Loaded precomputed target vectors from', DATA_PATH)\n",
    "else:\n",
    "    print('Precomputed vectors not found; computing embeddings (this may take a few minutes)')\n",
    "    # rebuild target_texts/target_meta from target_schema\n",
    "    target_texts = []\n",
    "    target_meta = []\n",
    "    for table in target_schema:\n",
    "        tdesc = table.get('desc','')\n",
    "        for attr in table.get('attributes',[]):\n",
    "            full = f\"{attr['name']} {attr.get('desc','')} {attr.get('type','')} {tdesc}\"\n",
    "            target_texts.append(full)\n",
    "            target_meta.append({'table': table['table'], 'attr': attr['name'], 'name': attr['name'], 'desc': attr.get('desc',''), 'full': full})\n",
    "    try:\n",
    "        target_vecs = embeddings.embed_documents(target_texts)\n",
    "    except Exception:\n",
    "        target_vecs = [encode(t) for t in target_texts]\n",
    "    # save for future\n",
    "    with open(DATA_PATH, 'wb') as f:\n",
    "        pickle.dump({'target_texts': target_texts, 'target_meta': target_meta, 'target_vecs': target_vecs}, f)\n",
    "    print('Saved embeddings to', DATA_PATH)\n",
    "\n",
    "# load ground truth pairs\n",
    "gt_pairs = load_ground_truth(gt_csv)\n",
    "gt_norm_pairs = [(normalize_text(s), normalize_text(t)) for s,t in gt_pairs]\n",
    "\n",
    "# helper\n",
    "def deterministic_map(retrieval_k, threshold):\n",
    "    preds = {}\n",
    "    for table_s in source_schema:\n",
    "        sdesc = table_s.get('desc','')\n",
    "        for attr_s in table_s.get('attributes',[]):\n",
    "            src = attr_s['name']\n",
    "            src_full = f\"{attr_s['name']} {attr_s.get('desc','')} {attr_s.get('type','')} {sdesc}\"\n",
    "            try:\n",
    "                q = embeddings.embed_query(src_full)\n",
    "            except Exception:\n",
    "                q = encode(src_full)\n",
    "            sims = [1 - cosine(q, v) for v in target_vecs]\n",
    "            idxs = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:retrieval_k]\n",
    "            best_idx = idxs[0]\n",
    "            best_sim = sims[best_idx]\n",
    "            if best_sim >= threshold:\n",
    "                preds[src] = target_meta[best_idx]['attr']\n",
    "            else:\n",
    "                preds[src] = None\n",
    "    eval_res = evaluate({normalize_text(k): (normalize_text(v) if v else None) for k,v in preds.items()}, set(gt_norm_pairs))\n",
    "    return eval_res, preds\n",
    "\n",
    "# grid\n",
    "retrieval_grid = [10, 20, 50, 100]\n",
    "threshold_grid = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85]\n",
    "best = None\n",
    "best_cfg = None\n",
    "start = time.time()\n",
    "for rk in retrieval_grid:\n",
    "    for thr in threshold_grid:\n",
    "        res, preds = deterministic_map(rk, thr)\n",
    "        f1 = res['f1']\n",
    "        print(f\"rk={rk}, thr={thr} -> F1={f1:.3f}, P={res['precision']:.3f}, R={res['recall']:.3f}\")\n",
    "        if best is None or f1 > best:\n",
    "            best = f1\n",
    "            best_cfg = (rk, thr, res)\n",
    "end = time.time()\n",
    "print('\\nBest deterministic config:', best_cfg[0], 'retrieval_k, threshold=', best_cfg[1], 'F1=', best)\n",
    "\n",
    "# set globals so the RAG cell will pick up the tuned values\n",
    "RETRIEVAL_K = best_cfg[0]\n",
    "TOP_N = min(50, RETRIEVAL_K)\n",
    "EMBEDDING_ACCEPT_THRESHOLD = best_cfg[1]\n",
    "FUZZY_THRESHOLD = globals().get('FUZZY_THRESHOLD', 0.2)\n",
    "THRESHOLD_SCORE = globals().get('THRESHOLD_SCORE', 0.1)\n",
    "print('Set globals: RETRIEVAL_K, TOP_N, EMBEDDING_ACCEPT_THRESHOLD')\n",
    "\n",
    "# show deterministic mapping for best config\n",
    "res, preds = deterministic_map(RETRIEVAL_K, EMBEDDING_ACCEPT_THRESHOLD)\n",
    "print('\\nDeterministic mapping evaluation with best config:')\n",
    "print(f\"  Ground-truth pairs: {len(gt_norm_pairs)}\")\n",
    "print(f\"  Predicted pairs: {len(res['pred_pairs'])}\")\n",
    "print(f\"  True positives: {len(res['tp'])}\")\n",
    "print(f\"  Precision: {res['precision']:.3f}\")\n",
    "print(f\"  Recall:    {res['recall']:.3f}\")\n",
    "print(f\"  F1:        {res['f1']:.3f}\")\n",
    "\n",
    "print('\\nTip: re-run the full RAG cell (the large one above) now — it will pick up the tuned globals and include descriptions in prompts.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
